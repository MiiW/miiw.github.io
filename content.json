{"meta":{"title":"Study4Fun","subtitle":"学而时习之","description":"闲思","author":"Mii Wang","url":"https://study4.fun","root":"/"},"pages":[{"title":"关于","date":"2022-05-21T07:32:38.292Z","updated":"2022-05-21T07:32:38.292Z","comments":false,"path":"about/index.html","permalink":"https://study4.fun/about/","excerpt":"","text":"杭州打工仔。"},{"title":"分类","date":"2022-05-21T07:32:38.292Z","updated":"2022-05-21T07:32:38.292Z","comments":false,"path":"categories/index.html","permalink":"https://study4.fun/categories/","excerpt":"","text":""},{"title":"友情链接","date":"2022-05-21T07:32:38.292Z","updated":"2022-05-21T07:32:38.292Z","comments":true,"path":"links/index.html","permalink":"https://study4.fun/links/","excerpt":"","text":""},{"title":"标签","date":"2022-05-21T07:32:38.292Z","updated":"2022-05-21T07:32:38.292Z","comments":false,"path":"tags/index.html","permalink":"https://study4.fun/tags/","excerpt":"","text":""}],"posts":[{"title":"Kubernetes容器监控原理和源码解析（一）——API和数据来源","slug":"Kubernetes容器监控原理和源码解析（一）——API和数据来源","date":"2022-05-13T14:54:00.000Z","updated":"2022-05-21T07:33:16.600Z","comments":true,"path":"2022-05-13-kubernetes-pod-monitoring-1/","link":"","permalink":"https://study4.fun/2022-05-13-kubernetes-pod-monitoring-1/","excerpt":"","text":"前言本系列主要基于 v1.24.0 版本的 Kubelet 部分源代码，进行 Kubernetes 中容器监控的底层原理介绍与代码分析。 Kubelet 中的监控 API在 Kubelet Server 提供的监控 API 中，大致可以分为两类：stats（统计数据）和 metrics（指标数据）。从命名和实际作用来看，前者提供了粗粒度的基础监控能力，目前用于各种内置组件；而后者用于持久化地进行细粒度的容器监控，主要提供给 Prometheus 等。 统计类 API在 v1.24.0 版本中，目前统计类接口仅包含/stats/summary，该接口提供了节点和 Pod 的统计信息。节点部分包括 CPU、内存、网络、文件系统、容器运行时、Rlimit 的统计。Pod 部分主要提供 Pod 相关的基础统计与卷、临时存储、进程统计外，主要还包括了各个容器的统计信息。容器部分值得关注的是，该接口中提供了用户自定义指标。方法实现如下： 12345678910111213141516171819func (h *handler) handleSummary(request *restful.Request, response *restful.Response) &#123; onlyCPUAndMemory := false ... if onlyCluAndMemoryParam, found := request.Request.Form[\"only_cpu_and_memory\"]; found &amp;&amp; len(onlyCluAndMemoryParam) == 1 &amp;&amp; onlyCluAndMemoryParam[0] == \"true\" &#123; onlyCPUAndMemory = true &#125; var summary *statsapi.Summary if onlyCPUAndMemory &#123; summary, err = h.summaryProvider.GetCPUAndMemoryStats() &#125; else &#123; forceStatsUpdate := false summary, err = h.summaryProvider.Get(forceStatsUpdate) &#125; ...&#125; 查看 SummaryProvider 的实现可以发现，该实际上就是对 stats.Provider 的封装。 1234567891011type SummaryProvider interface &#123; Get(updateStats bool) (*statsapi.Summary, error) GetCPUAndMemoryStats() (*statsapi.Summary, error)&#125;type summaryProviderImpl struct &#123; kubeletCreationTime metav1.Time systemBootTime metav1.Time provider Provider&#125; 在 handler 的处理逻辑中，它提供了一个只返回 CPU 和 Memory 信息的选项 onlyCPUAndMemory，如果只关心 cpu 和内存信息，通过此选项可以去除多余的统计信息，metrics-server(_&lt; 0.6.0 版本_)中就默认设置了该值。该部分需要注意的是，如果获取的是完整信息，那么监控信息是从缓存中获取的，这里特指 CPU 中的 NanoCoreUsage 不会更新，只有调用 onlyCPUAndMemory 才会将该值更新。如果基于接口做定制开发需要将 forceStatsUpdate 修改为 true 以保证 NanoCoreUsage 的准确性。此外，在最新版本的 0.6.x 版本的 metrics-server 中已经不再依赖该接口，而是采用了 Kubelet 中的/metrics/resource 接口进行资源的监控，在老版本集群中部署 metrics-server 时需要注意不兼容问题。 指标类 APIKubelet Server 提供的指标类 API 目前包括以下四个： /metrics：提供 kubelet 自身相关的一些监控，包括：apiserver 请求、go gc/内存/线程相关、kubelet 子模块关键信息、client-go 等指标 /metrics/cadvisor：提供 Pod/容器监控信息 /metrics/probes：提供对容器 Liveness/Readiness/Startup 探针的指标数据 /metrics/resource：提供 Pod/容器的 CPU 用量、wss 内存、启动时间基础指标数据 上述四个接口返回的指标信息默认都是 Promtheus 格式。一般来说，指标想要转化为 Promtheus 格式需要实现 Prometheus client 的 Registerer 和 Gatherer 接口，而在 K8s 中对应的封装实现就是 KubeRegistry。这里我们先跳过 Prometheus client 的实现原理和其他非容器指标相关的实现，而是来看看容器指标数据是如何获取到并转化返回的。 1234567891011121314151617181920r := compbasemetrics.NewKubeRegistry()includedMetrics := cadvisormetrics.MetricSet&#123; // 指标白名单设置 ...&#125;cadvisorOpts := cadvisorv2.RequestOptions&#123; IdType: cadvisorv2.TypeName, // 每次仅返回一条数据 Count: 1, Recursive: true,&#125;r.RawMustRegister(metrics.NewPrometheusCollector(prometheusHostAdapter&#123;s.host&#125;, containerPrometheusLabelsFunc(s.host), includedMetrics, clock.RealClock&#123;&#125;, cadvisorOpts))r.RawMustRegister(metrics.NewPrometheusMachineCollector(prometheusHostAdapter&#123;s.host&#125;, includedMetrics))s.restfulCont.Handle(cadvisorMetricsPath, compbasemetrics.HandlerFor(r, compbasemetrics.HandlerOpts&#123;ErrorHandling: compbasemetrics.ContinueOnError&#125;), ) 在 Prometheus Client 的注册逻辑里，返回数据需要实现指标收集器（Collector）部分。这里可以看到，Kubelet 提供了两种数据收集器，一部分是 Pod/容器的指标收集器，另一部分是节点指标收集器。 12345678910111213141516171819202122232425func NewPrometheusMachineCollector(i infoProvider, includedMetrics container.MetricSet) *PrometheusMachineCollector &#123; c := &amp;PrometheusMachineCollector&#123; infoProvider: i, errors: prometheus.NewGauge(prometheus.GaugeOpts&#123; Namespace: \"machine\", Name: \"scrape_error\", Help: \"1 if there was an error while getting machine metrics, 0 otherwise.\", &#125;), machineMetrics: []machineMetric&#123; &#123; name: \"machine_cpu_physical_cores\", help: \"Number of physical CPU cores.\", valueType: prometheus.GaugeValue, getValues: func(machineInfo *info.MachineInfo) metricValues &#123; return metricValues&#123;&#123;value: float64(machineInfo.NumPhysicalCores), timestamp: machineInfo.Timestamp&#125;&#125; &#125;, &#125;, // 其他指标的类似实现 ... &#125;...) &#125; return c&#125; 可以看到收集器实际上主要是通过 getValues 方法把 infoProvider 提供的数据结构从原始结构转成 Prometheus 中的指标，同时每个指标需要定义其名称、类型以及描述。节点和 Pod/容器对应的原始数据结构都定义在 cAdvisor 的 API Spec 中，即 MachineInfo 和 ContainerStats。 12345678type infoProvider interface &#123; // GetRequestedContainersInfo gets info for all requested containers based on the request options. GetRequestedContainersInfo(containerName string, options v2.RequestOptions) (map[string]*info.ContainerInfo, error) // GetVersionInfo provides information about the version. GetVersionInfo() (*info.VersionInfo, error) // GetMachineInfo provides information about the machine. GetMachineInfo() (*info.MachineInfo, error)&#125; 而从上面 KubeRegistry 的注册逻辑中可以看到，infoProvider 的实现又需要通过 prometheusHostAdapter 进行一次转换，其转换前的实现接口为 HostInterface。 12345678910111213type prometheusHostAdapter struct &#123; host HostInterface&#125;func (a prometheusHostAdapter) GetRequestedContainersInfo(containerName string, options cadvisorv2.RequestOptions) (map[string]*cadvisorapi.ContainerInfo, error) &#123; return a.host.GetRequestedContainersInfo(containerName, options)&#125;func (a prometheusHostAdapter) GetVersionInfo() (*cadvisorapi.VersionInfo, error) &#123; return a.host.GetVersionInfo()&#125;func (a prometheusHostAdapter) GetMachineInfo() (*cadvisorapi.MachineInfo, error) &#123; return a.host.GetCachedMachineInfo()&#125; 在 HostInterface 的定义中，查看监控相关的部分接口，prometheusHostAdapter 需要的 VersionInfo 和 MachineInfo 是由 Kubelet 实现的，而 GetRequestedContainersInfo 即容器相关监控信息的接口则由前面提到的 stats.Provider 中实现。 123456789type HostInterface interface &#123; stats.Provider GetVersionInfo() (*cadvisorapi.VersionInfo, error) GetCachedMachineInfo() (*cadvisorapi.MachineInfo, error) GetRunningPods() ([]*v1.Pod, error) GetHostname() string // 省略监控无关接口&#125; 监控数据提供方 Stats ProviderProvider 定义基于上一节分析，可以了解到 Kubelet Server 中返回的监控信息，无论是统计类信息还是指标类信息，容器相关的部分都是通过 stats.Provider 获取的。那么我们看看 stats.Provider 部分是如何定义的： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647type Provider interface &#123; // ListPodStats 返回Pod管理的容器统计信息 ListPodStats() ([]statsapi.PodStats, error) // ListPodCPUAndMemoryStats 返回Pod管理的容器统计信息（CPU/内存部分） ListPodCPUAndMemoryStats() ([]statsapi.PodStats, error) // ListPodStatsAndUpdateCPUNanoCoreUsage 返回Pod管理的容器统计信息，这个方法会强制更新cpu的NanoCoreUsage信息，主要用于部分未内部集成cAdvisor的CRI Runtime实现。详见：https://github.com/kubernetes/kubernetes/issues/72788 ListPodStatsAndUpdateCPUNanoCoreUsage() ([]statsapi.PodStats, error) // ImageFsStats 返回镜像文件系统的统计信息 ImageFsStats() (*statsapi.FsStats, error) // GetCgroupStats 通过指定的cgroup名称返回统计信息及网络用量 GetCgroupStats(cgroupName string, updateStats bool) (*statsapi.ContainerStats, *statsapi.NetworkStats, error) // GetCgroupCPUAndMemoryStats 通过指定的cgroupName返回CPU和内存统计信息 GetCgroupCPUAndMemoryStats(cgroupName string, updateStats bool) (*statsapi.ContainerStats, error) // RootFsStats 返回节点根分区的统计信息 RootFsStats() (*statsapi.FsStats, error) // GetContainerInfo 通过Pod Uid返回该Pod管理的容器的指标信息 GetContainerInfo(podFullName string, uid types.UID, containerName string, req *cadvisorapi.ContainerInfoRequest) (*cadvisorapi.ContainerInfo, error) // GetRawContainerInfo 通过容器名返回容器的指标信息，如果开启了subcontainers选项，则该方法会返回所有子容器的指标信息 GetRawContainerInfo(containerName string, req *cadvisorapi.ContainerInfoRequest, subcontainers bool) (map[string]*cadvisorapi.ContainerInfo, error) // GetRequestedContainersInfo 通过容器名返回容器的指标信息，同事提供了一些cAdvisor特定的可选参数 GetRequestedContainersInfo(containerName string, options cadvisorv2.RequestOptions) (map[string]*cadvisorapi.ContainerInfo, error) // GetPodByName 通过Pod的命名空间和名称返回具体的Pod信息 GetPodByName(namespace, name string) (*v1.Pod, bool) // GetNode 返回节点规格信息 GetNode() (*v1.Node, error) // GetNodeConfig 返回节点配置信息 GetNodeConfig() cm.NodeConfig // ListVolumesForPod 通过Pod Uid返回对应Pod使用的卷统计信息 ListVolumesForPod(podUID types.UID) (map[string]volume.Volume, bool) // ListBlockVolumesForPod 通过Pod Uid返回对应Pod使用的块设备卷统计信息 ListBlockVolumesForPod(podUID types.UID) (map[string]volume.BlockVolume, bool) // GetPods 返回节点上运行的所有Pod的信息 GetPods() []*v1.Pod // RlimitStats 返回系统的rlimit统计 RlimitStats() (*statsapi.RlimitStats, error) // GetPodCgroupRoot 返回管理所有Pod的根Cgroup节点路径 GetPodCgroupRoot() string // GetPodByCgroupfs 通过cgroup路径名查找并返回Pod信息 GetPodByCgroupfs(cgroupfs string) (*v1.Pod, bool)&#125; 通过上面 Provider 的接口划分，我们大致了解了 Kubelet 在监控方面提供的能力，主要包括容器、Pod、节点、文件系统的统计信息或指标信息等。其中不少接口都用到了 cgroup。cgroup（control group）是 Linux 内核中用于限制、记录和隔离一组进程的资源使用（CPU、内存、磁盘 I/O、网络等）的模块，它与 namespace 一起作为基石构成了容器基础技术的实现。绝大部分场景下，大家对他比较熟悉的点在于其发挥的资源划分和限制能力，例如 K8S 中 Pod 与 Container 的 CPU/内存资源 Limit。除了资源限制能力外，实际上 cgroup 每个子系统还具有对应的统计能力。以 CPU 子系统为例，它可以统计 CPU 在用户空间和内核空间的运行时长分配、设置 Limit 后的 CPU 限流次数和限流总时长等：Cgroup 是以树形结构来划分和组织系统内的各个进程的，Kubelet 在运行时会创建一个 kubepods 的叶节点。而根据 Pod 的 QoS 等级划分，不同 Pod 被放在不同的 QoS 叶节点下。对应的，容器本身作为 Pod 管理的最小单元，其被划分在 Pod 的叶节点以下（_需开启 containerd 中 runc 的 SystemdCgroup 选项，否则 container 由 containerd cgroup 单独管理_）。大致了解了 Cgroup 之后，细心的你就能想明白在 Provider 中不太容易理的 GetRawContainerInfo 和 GetRequestedContainersInfo 接口的用法，即参数中的 Container 指的不仅是被 Pod 管理的 Container，其实指的也是 Cgroup 的叶节点。它可以是 Container/Pod/QoS/Kubepods，甚至是节点本身（即”/“）。这也能解释为什么会有 subcontainers 这个参数，毕竟 Container 作为最小运行单元肯定是不存在子容器这个说法的。subcontainers 可以直接理解为是否包含子节点的信息，GetRawContainerInfo 接口实际上是提供了当前节点与子节点的统计信息。 Provider 初始化provider 初始化包含在 Kubelet 的初始化流程中，通过 useLegacyCadvisorStats 开关，Kubelet 会进行会在 CadvisorStatsProvider 和 CRIStatsProvider 中选择一种作为实际实现。 123456789101112131415161718192021222324252627282930func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, ... )&#123; // 其他初始化流程 ... hostStatsProvider := stats.NewHostStatsProvider(kubecontainer.RealOS&#123;&#125;, func(podUID types.UID) string &#123; return getEtcHostsPath(klet.getPodDir(podUID)) &#125;) if kubeDeps.useLegacyCadvisorStats &#123; klet.StatsProvider = stats.NewCadvisorStatsProvider( klet.cadvisor, klet.resourceAnalyzer, klet.podManager, klet.runtimeCache, klet.containerRuntime, klet.statusManager, hostStatsProvider) &#125; else &#123; klet.StatsProvider = stats.NewCRIStatsProvider( klet.cadvisor, klet.resourceAnalyzer, klet.podManager, klet.runtimeCache, kubeDeps.RemoteRuntimeService, kubeDeps.RemoteImageService, hostStatsProvider,utilfeature.DefaultFeatureGate.Enabled(features.DisableAcceleratorUsageMetrics) utilfeature.DefaultFeatureGate.Enabled(features.PodAndContainerStatsFromCRI)) &#125;&#125; 对比两者的初始化方法可以分析得出其主要的数据来源都是 cadvisor 和 resourceAnalyzer，其他参数是主要作为一些辅助选项。resourceAnalyzer 主要提供了节点资源消耗的统计，除了文件系统相关的统计，其主要实现还是我们之前提到的 SummaryProvider。那么基本可以得出一个结论，无论是哪种实现，容器相关的监控都主要来自于 cAdvisor。 总结本篇文章中，我们主要了解了 Kubelet Server 在对外提供的监控 API 中统计类和指标类的划分，并了解到容器监控的部分主要由 Stats Provider 定义。同时，我们也知道了目前 Kubelet 中对于 Stats Provider 的实现主要有两种——CadvisorStatsProvider 和 CRIStatsProvider。接下来的一篇文章，我们会对这两种实现进行深入的对比分析，并搞清楚两种 Provider 的实现是如何使用 cAdvisor 接口的。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"},{"name":"Pod","slug":"Pod","permalink":"https://study4.fun/tags/Pod/"},{"name":"容器","slug":"容器","permalink":"https://study4.fun/tags/%E5%AE%B9%E5%99%A8/"},{"name":"监控","slug":"监控","permalink":"https://study4.fun/tags/%E7%9B%91%E6%8E%A7/"}]},{"title":"《KVM实战》读书笔记","slug":"《KVM实战》读书笔记(施工中）","date":"2021-06-25T14:39:00.000Z","updated":"2022-05-21T07:33:16.700Z","comments":true,"path":"2021-06-25-kvm-in-action-notes/","link":"","permalink":"https://study4.fun/2021-06-25-kvm-in-action-notes/","excerpt":"","text":"虚拟化分类技术手段 软件虚拟化 硬件虚拟化 含义 纯软件的环境模拟执行客户机的指令，如 QEMU 硬件本身提供能力让客户机指令独立执行，不需要 VMM 截获重定向，如 Intel VT 实现 二进制翻译（不同指令集的二进制代码转换） 1.（x86 实现）提供略微受限的硬件运行环境（non-root mode），无需翻译再执行 2.VMM 运行在 root mode，具备硬件的完整访问控制权限；仅在部分情况下，客户机指令需要被 VMM 截获处理再返回 non-root mode 优缺点 虚拟化性能差，软件复杂度高；支持各种平台（二进制翻译支持） 性能接近于原生系统，简化了 VMM 的设计结构 虚拟化程度半虚拟化： 客户机意识到自己运行在虚拟化环境内，需做相应的修改配合 VMM 性能提升，VMM 软件复杂度降低；不太依赖硬件虚拟化的支持，支持跨平台 virtio，宿主机/VMM 和客户机需要相应的安装驱动 全虚拟化： 客户机 OS 无改动，需要模拟完整的、和物理平台一模一样的平台提供给客户机 增加了 VMM 的复杂度，软件实现的全虚拟化性能差 硬件虚拟化辅助的全虚拟化性能反超半虚拟化 软件框架判断标准：VMM 是直接位于硬件之上还是在一个宿主操作系统上Type1： native/bare-metal hypervisor，直接控制硬件资源及客户机 Xen, VMware Esx Type2： 运行在宿主机 OS 上，通常就是以进程的形式存在 KVM, VMWare Workstation 硬件虚拟化(TODO) CPU 虚拟化内存虚拟化I/O 虚拟化KVM 介绍定位 采用硬件虚拟化的全虚拟化解决方案 基于内核，通过加载模块使内核本身成为 Hypervisor Type2 Hypervisor 特性内存管理 客户机物理内存就是宿主机内普通进程的虚拟内存，这意味着 linux 内存管理的机制全部可以应用到客户机内存管理上 早期实现：GVA-&gt;GPA-&gt;HVA-&gt;HPA；GVA-&gt;GPA：页表，GPA-&gt;HPA：影子页表 现在的实现：通过硬件识别的数据结构算出 GPA 到 HPA 的转换关系，如 EPT/NPT 存储和镜像 可使用 linux 支持的任意存储来存虚拟机镜像，支持按需分配、稀疏存储 原生磁盘格式为 qcow2，支持快照、压缩、加密 其他 实时迁移，用户透明，支持状态存储及恢复 支持混合虚拟化（virtio，pci passthrough） 资源可伸缩，95%原生性能 功能框架 概述 一个 Guest VM 对应 Host 中的一个 qemu 进程，一个 Guest vCpu 对应进程中的一个 qemu 线程 IO 处理线程是单独的，在一个线程组内 客户机（进程）由内核调度，借用进程调度的手段实现不同客户机的权限限定和优先级等功能 QEMU 实现硬件设备模拟（透传除外），截获并对物理设备驱动进行操作 核心组成KVM 内核模块 主要负责 CPU 和内存的虚拟化，所有 Guest 与唯一的内核模块进行交互 分为处理器架构无关(kvm)和有关(kvm_intel)的两个部分 QEMU 用户态工具 提供设备模拟的功能，ioctl syscall 与内核态 kvm 模块交互 qemu 本身具备完整的虚拟机实现（cpu/mem 虚拟化） 支持 virtio 协议的设备模拟，实现了 virtio 的虚拟化后端 其他相关组件 vhost-net：替代 virtio-net open vswitch：多层虚拟交换机 dpdk 网络应用中数据包的高性能处理，运行在用户空间，绕过内核协议栈对数据包的处理 提升小包等网络处理能力，可与 vhost-uset 结合使用 spdk：高存储性能支持，驱动运行在用户态实现零拷贝、轮询替代传统的中断模式、无锁设计 管理工具 libvirt：kvm 虚拟化管理的工具和应用程序接口，事实上的虚拟化接口标准 virsh：命令行工具 virt-manager：图形化管理软件 openstack：云管理平台 KVM 操作准备1234567891011121314151617181920# bios中确认打开Intel VT/VT-d# cpu标志确认虚拟化支持grep -E \"svm|vmx\" /proc/cpuinfo# kvm模块加载modprobe kvmmodprobe kvm_intellsmod | grep kvm# 检查控制接口文件ls -l /dev/kvm# 默认安装的qemu版本比较老，编译安装最新版本的 git clone git://git.qemu.org/qemu.git# 编译配置所需 yum install -y python3 ninja-build gcc# 进行配置，只安装x86_64客户机支持 ./configure --target-list=x86_64-softmmu 旧版本需要安装 qemu-kvm，新版本 qemu 增加 –enable-kvm 选项即可 使用12345678# 以raw格式创建一个大小20G的盘，默认按需分配qemu-img create -f raw rhel7.img 20G# ls -lh | grep rhel7.img-rw-r--r-- 1 root root 20G Jun 28 11:57 rhel7.img# du -sh rhel7.img，按需分配，实际占用为00 rhel7.img libvirt定位 对 KVM 虚拟机进行管理的工具和应用程序接口 也提供了对虚拟化网络和存储的管理 主要作为连接底层 Hypervisor 和上层应用程序的一个中间适配层 交互框架 通过基于驱动程序的架构实现多种 Hypervisor 的支持 屏蔽底层 Hypervisor 的细节，为上层提供统一的稳定的 API 组成和功能组成应用接口编程接口库，守护进程（libvirtd)，命令行管理工具(virsh)libvirtd：可运行在 root 或普通用户权限下，root 用户权限支持所有管理操作，普通用户则受限 功能 域管理：域生命周期管理，多种设备的热插拔 远程节点管理：支持多种网络远程传输类型连接运行了 libvirtd 的物理节点 存储管理：管理不同类型的存储，支持远程管理 网络管理：管理物理和逻辑网络接口 提供稳定可靠高效的应用程序接口 XML 配置CPU 配置vcpu：cpu 个数 vcpu.cpuset: 指定允许执行的物理 CPU features：CPU 或其他硬件特性的开关cpu.mode: custom:个性化设置 host-model:根据物理 CPU 特性选择一个最接近的标准 CPU 型号 host-passthrough：暴露 CPU 物理特性给 Guest cputune：绑核、CPU 时间加权配置等等 内存配置memory:可使用内存的最大值currentMemory：启动时分配的内存devices.memballoon:内存 balloning 调节 Guest OS 和 Boot 配置os.tpye:GuestOS 类型，kvm 中为 hvmos.boot：guest vm 启动设备配置 网络配置interface.type: bridge，桥接网络；network，NAT 网络；user，用户模式网络；hostdev，设备直接分配给 Guest 存储配置disk.type: file/block（块设备）/dir/network,磁盘来源类型disk.device:floppy/disk（默认值）/cdrom/lun，Guest 内使用disk.source:磁盘来源，和 type 配合使用disk.driver:hypervisor 提供的磁盘驱动，name=qemudisk.target:暴露给 Guest 的总线类型和设备名称disk.address:Guest 内的 PCI 总线地址 其他配置域配置domain.type：虚拟化类型,xen/kvm/qemu/lxc…domain.id: guest 唯一 iddomain.name/uuid/…: 元数据 QEMU 模拟器配置device.emulator:使用的设备模型模拟器，绝对路径 图形显示devices.graphics:连接 Guest 的图形显示方式，vnc/sdl/rdp… 声卡/显卡devices.sound:模拟声卡devices.video:模拟显卡，vram 代表显存，heads 屏幕序号 串口和控制台devices.serial:串口，target 指 Guest 内串口编号，source 指 Host 内的虚拟 pts 终端号devices.console:控制台配置，type=serial，Guest 内类型 输入设备devices.input：qemu 模拟的输入设备，mouse/keyboard/tablet PCI 控制器domain.controller:类型有 usb/pci/virtio-serial libvirt apiHypervisor 连接（virConnect）域管理（virDomain）节点管理（virNode）网络管理（virNetowrk）存储卷管理（virStorageVol）：域的镜像文件管理存储池管理（virStoragePool）：本地、网络共享、iSCSI 文件系统以及 LVM 分区管理事件管理（virEvent）：事件机制支持，注册后事件发生后可收到通知数据流管理（virStream）：数据流传输 libvirt uri本地格式：driver[+transport]:///[path][extral-param]示例：qemu:///sessionqemu+unix:///sessionqemu:///systemqemu+unix:///system 远程格式：driver[+transport]://[uesr@][host][:port]/[path][extral-param]示例：qemu+ssh://root@example.com/systemqemu+ssh://user@example.com/sessionqemu://example.com/system(TLS 连接)qemu+tcp//example.com/system(非加密 TCP) 其他工具virsh: 相关操作命令[]virt manager:图形化管理[]virt viewer:图形化控制台[] 其他注意点新版本 qemu 编译安装的时候依赖 gcc 7.5+版本 问题设备模型桥接网络和 NAT 网络的区别用户模式网络：qemu 软件模拟的网络协议栈，宿主机中没有虚拟网络接口连接到网桥istio-iptables Refhttps://www.cnblogs.com/music-liang/p/12900457.html","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"KVM","slug":"KVM","permalink":"https://study4.fun/tags/KVM/"}]},{"title":"Kubelet Image GC原理分析","slug":"Kubelet Image GC原理分析","date":"2021-04-25T07:48:00.000Z","updated":"2022-05-21T07:33:16.780Z","comments":true,"path":"2021-04-25-analysis-of-kubelet-imagegc/","link":"","permalink":"https://study4.fun/2021-04-25-analysis-of-kubelet-imagegc/","excerpt":"","text":"Image GC 是什么？Image GC 是 kubelet 的镜像清理功能，用于在磁盘空间不足的情况下清除不需要的镜像，释放磁盘空间，保证 Pod 能正常启动运行。 Image GC 如何使用？Kubelet 默认开启，通过 kubele 启动配置中的 ImageGCPolicy 控制。ImageGCPolicy 有三个设置参数： ImageGCHighThresholdPercent：触发 gc 的阈值，超过该值将会执行 gc，设置为 100 时，gc 不启动。 ImageGCLowThresholdPercent：ImageGC 执行空间空间的目标值，gc 触发后，将会将磁盘占用率降至该值以下； ImageMinimumGCAge：最短 GC 年龄（即距离首次被探测到的间隔），小于该阈值时不会被 gc。 源码分析ImageGC 的初始化与启动在 kubelet 启动时，ImageGC 的启动在 BirthCry 执行完成之后。 12345678910111213141516171819202122232425262728293031323334func (kl *Kubelet) StartGarbageCollection() &#123; loggedContainerGCFailure := false // container gc流程，省略 ... // ImageGCHighThresholdPercent设置为100时，关闭image gc if kl.kubeletConfiguration.ImageGCHighThresholdPercent == 100 &#123; klog.V(2).Infof(\"ImageGCHighThresholdPercent is set 100, Disable image GC\") return &#125; prevImageGCFailed := false go wait.Until(func() &#123; if err := kl.imageManager.GarbageCollect(); err != nil &#123; if prevImageGCFailed &#123; klog.Errorf(\"Image garbage collection failed multiple times in a row: %v\", err) // Only create an event for repeated failures kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ImageGCFailed, err.Error()) &#125; else &#123; klog.Errorf(\"Image garbage collection failed once. Stats initialization may not have completed yet: %v\", err) &#125; prevImageGCFailed = true &#125; else &#123; var vLevel klog.Level = 4 if prevImageGCFailed &#123; vLevel = 1 prevImageGCFailed = false &#125; klog.V(vLevel).Infof(\"Image garbage collection succeeded\") &#125; &#125;, ImageGCPeriod, wait.NeverStop)&#125; 可以看到，ImageGC 由单独的协程执行，默认的执行间隔为五分钟。当 ImageGC 首次执行失败时会打印日志，而重复失败后，会记录一个 ImageGCFailed 的事件。这意味着可以通过配置日志或者告警了解 GC 是否正常运行。接下来看看 ImageGCManager 的具体实现。 1234567891011121314151617181920212223242526272829303132333435363738type ImageGCManager interface &#123; // Applies the garbage collection policy. Errors include being unable to free // enough space as per the garbage collection policy. GarbageCollect() error // Start async garbage collection of images. Start() GetImageList() ([]container.Image, error) // Delete all unused images. DeleteUnusedImages() error&#125;func NewImageGCManager(runtime container.Runtime, statsProvider StatsProvider, recorder record.EventRecorder, nodeRef *v1.ObjectReference, policy ImageGCPolicy, sandboxImage string) (ImageGCManager, error) &#123; // Validate policy. if policy.HighThresholdPercent &lt; 0 || policy.HighThresholdPercent &gt; 100 &#123; return nil, fmt.Errorf(\"invalid HighThresholdPercent %d, must be in range [0-100]\", policy.HighThresholdPercent) &#125; if policy.LowThresholdPercent &lt; 0 || policy.LowThresholdPercent &gt; 100 &#123; return nil, fmt.Errorf(\"invalid LowThresholdPercent %d, must be in range [0-100]\", policy.LowThresholdPercent) &#125; if policy.LowThresholdPercent &gt; policy.HighThresholdPercent &#123; return nil, fmt.Errorf(\"LowThresholdPercent %d can not be higher than HighThresholdPercent %d\", policy.LowThresholdPercent, policy.HighThresholdPercent) &#125; im := &amp;realImageGCManager&#123; runtime: runtime, policy: policy, imageRecords: make(map[string]*imageRecord), statsProvider: statsProvider, recorder: recorder, nodeRef: nodeRef, initialized: false, sandboxImage: sandboxImage, &#125; return im, nil&#125; ImageGCManager 的接口非常简单，只有四个方法： GarbageCollect：根据定义的 ImageGCPolicy 执行具体的清理动作； Start：异步地收集镜像信息； GetImageList：获取缓存中的镜像列表； DeleteUnusedImages：删除未使用的镜像。 ImageGCManager 在初始化时会校验 Policy 的参数合法性，然后传递运行时、监控、事件等参数。然后看看 Start 方法的逻辑： 1234567891011121314151617181920212223242526func (im *realImageGCManager) Start() &#123; go wait.Until(func() &#123; // Initial detection make detected time \"unknown\" in the past. var ts time.Time if im.initialized &#123; ts = time.Now() &#125; _, err := im.detectImages(ts) if err != nil &#123; klog.Warningf(\"[imageGCManager] Failed to monitor images: %v\", err) &#125; else &#123; im.initialized = true &#125; &#125;, 5*time.Minute, wait.NeverStop) // Start a goroutine periodically updates image cache. go wait.Until(func() &#123; images, err := im.runtime.ListImages() if err != nil &#123; klog.Warningf(\"[imageGCManager] Failed to update image list: %v\", err) &#125; else &#123; im.imageCache.set(images) &#125; &#125;, 30*time.Second, wait.NeverStop)&#125; ImageGCManager 的 Start 方法会启动两个协程。在第一个协程内，每隔五分钟 Manager 会检查一次镜像。一旦完成一次，Manager 的状态就会被标记为已初始化。另一个协程每隔 30 秒会从容器运行时获取所有的镜像信息，更新到缓存的镜像列表中。 镜像信息的检测和维护那么，Manager 时如何检测镜像的呢？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263func (im *realImageGCManager) detectImages(detectTime time.Time) (sets.String, error) &#123; imagesInUse := sets.NewString() // Always consider the container runtime pod sandbox image in use imageRef, err := im.runtime.GetImageRef(container.ImageSpec&#123;Image: im.sandboxImage&#125;) if err == nil &amp;&amp; imageRef != \"\" &#123; imagesInUse.Insert(imageRef) &#125; images, err := im.runtime.ListImages() if err != nil &#123; return imagesInUse, err &#125; pods, err := im.runtime.GetPods(true) if err != nil &#123; return imagesInUse, err &#125; // Make a set of images in use by containers. for _, pod := range pods &#123; for _, container := range pod.Containers &#123; klog.V(5).Infof(\"Pod %s/%s, container %s uses image %s(%s)\", pod.Namespace, pod.Name, container.Name, container.Image, container.ImageID) imagesInUse.Insert(container.ImageID) &#125; &#125; // Add new images and record those being used. now := time.Now() currentImages := sets.NewString() im.imageRecordsLock.Lock() defer im.imageRecordsLock.Unlock() for _, image := range images &#123; klog.V(5).Infof(\"Adding image ID %s to currentImages\", image.ID) currentImages.Insert(image.ID) // New image, set it as detected now. if _, ok := im.imageRecords[image.ID]; !ok &#123; klog.V(5).Infof(\"Image ID %s is new\", image.ID) im.imageRecords[image.ID] = &amp;imageRecord&#123; firstDetected: detectTime, &#125; &#125; // Set last used time to now if the image is being used. if isImageUsed(image.ID, imagesInUse) &#123; klog.V(5).Infof(\"Setting Image ID %s lastUsed to %v\", image.ID, now) im.imageRecords[image.ID].lastUsed = now &#125; klog.V(5).Infof(\"Image ID %s has size %d\", image.ID, image.Size) im.imageRecords[image.ID].size = image.Size &#125; // Remove old images from our records. for image := range im.imageRecords &#123; if !currentImages.Has(image) &#123; klog.V(5).Infof(\"Image ID %s is no longer present; removing from imageRecords\", image) delete(im.imageRecords, image) &#125; &#125; return imagesInUse, nil&#125; 检测镜像的目的是找出正在使用的镜像，防止在 GC 执行的过程中被清理。同时，在此过程中，镜像的清理需要参考一些信息，这些信息也会在检测的过程中更新。首先，Sandbox 镜像是一定会被判定为正在使用的镜像。接着会将所有 Pod 的所有正在运行中的容器使用的 image 加到正在使用的镜像列表中。注意，即使 Pod 有容器需要该镜像，但是该容器未处于 Running 状态，其对应的镜像也会被清理。选出正在使用（即不会被清理）的镜像之后，会将容器运行时中获取到的镜像列表信息更新到 Manager 维护的镜像列表记录中。查询所有新获取的镜像列表信息进行遍历，分为以下几步： 如果是第一次被记录，那么更新该镜像的首次被探测时间为本轮探测的事件 如果被前面一步被判定为“正在使用的镜像”，那么它的最新使用事件会被刷新为当前时间 刷新获取到的镜像的大小 最后，如果某个镜像已经不在容器运行时返回的镜像列表中，就会被移出 Manager 缓存的镜像探测记录。 ImageGC 的具体执行ImageGCManager 的核心方法就是 GarbageCollect 了，主要步骤如下：首先获取 Image 对应的 Filesystem 占用信息，根据启动的配置计算出用量百分比以及需要释放的空间大小，然后开始释放。如果实际释放的空间小于目标大小，会记录 FreeDiskSpaceFailed 的 Warnning 事件。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546func (im *realImageGCManager) GarbageCollect() error &#123; // Get disk usage on disk holding images. fsStats, err := im.statsProvider.ImageFsStats() if err != nil &#123; return err &#125; var capacity, available int64 if fsStats.CapacityBytes != nil &#123; capacity = int64(*fsStats.CapacityBytes) &#125; if fsStats.AvailableBytes != nil &#123; available = int64(*fsStats.AvailableBytes) &#125; if available &gt; capacity &#123; klog.Warningf(\"available %d is larger than capacity %d\", available, capacity) available = capacity &#125; // Check valid capacity. if capacity == 0 &#123; err := goerrors.New(\"invalid capacity 0 on image filesystem\") im.recorder.Eventf(im.nodeRef, v1.EventTypeWarning, events.InvalidDiskCapacity, err.Error()) return err &#125; // If over the max threshold, free enough to place us at the lower threshold. usagePercent := 100 - int(available*100/capacity) if usagePercent &gt;= im.policy.HighThresholdPercent &#123; amountToFree := capacity*int64(100-im.policy.LowThresholdPercent)/100 - available klog.Infof(\"[imageGCManager]: Disk usage on image filesystem is at %d%% which is over the high threshold (%d%%). Trying to free %d bytes down to the low threshold (%d%%).\", usagePercent, im.policy.HighThresholdPercent, amountToFree, im.policy.LowThresholdPercent) freed, err := im.freeSpace(amountToFree, time.Now()) if err != nil &#123; return err &#125; if freed &lt; amountToFree &#123; err := fmt.Errorf(\"failed to garbage collect required amount of images. Wanted to free %d bytes, but freed %d bytes\", amountToFree, freed) im.recorder.Eventf(im.nodeRef, v1.EventTypeWarning, events.FreeDiskSpaceFailed, err.Error()) return err &#125; &#125; return nil&#125; 算出需要释放空间后是删除的镜像是怎么决定的呢？在开始执行清理时，会执行我们上面介绍的镜像探测过程。在完成镜像探测后，我们的得到的 imagesInUse 包括了 Sanbox 镜像以及 Pod 内正在与运行中容器使用的镜像。接下来，要选出清理的目标镜像，存放清理目标的数据结构叫 evictionInfo，它存放了所有不在 imagesInUse 列表内的镜像记录。接着会将这些镜像记录按照 最后使用时间 和 首次探测时间 进行一次排序，即按照 LRU 规则将最后一次使用时间较早和探测事件较早的镜像排在前面。排序完之后，会遍历所有这些镜像：如果是镜像最后一次使用事件没有删除触发时间早（即刚刚刷新了最后使用时间），则不会删除。同时，如果该镜像首次被探测到的时间差小于配置的最小 GC 间隔（即刚加入到缓存记录中），也不会删除。否则，就会依序删除这些镜像，删除完之后会从探测记录中删除该镜像同时累加 已经释放的空间值 spaceFreed。如果 spaceFreed 不小于目标释放的空间，则本轮的清理正常结束。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162func (im *realImageGCManager) freeSpace(bytesToFree int64, freeTime time.Time) (int64, error) &#123; imagesInUse, err := im.detectImages(freeTime) if err != nil &#123; return 0, err &#125; im.imageRecordsLock.Lock() defer im.imageRecordsLock.Unlock() // Get all images in eviction order. images := make([]evictionInfo, 0, len(im.imageRecords)) for image, record := range im.imageRecords &#123; if isImageUsed(image, imagesInUse) &#123; klog.V(5).Infof(\"Image ID %s is being used\", image) continue &#125; images = append(images, evictionInfo&#123; id: image, imageRecord: *record, &#125;) &#125; sort.Sort(byLastUsedAndDetected(images)) // Delete unused images until we've freed up enough space. var deletionErrors []error spaceFreed := int64(0) for _, image := range images &#123; klog.V(5).Infof(\"Evaluating image ID %s for possible garbage collection\", image.id) // Images that are currently in used were given a newer lastUsed. if image.lastUsed.Equal(freeTime) || image.lastUsed.After(freeTime) &#123; klog.V(5).Infof(\"Image ID %s has lastUsed=%v which is &gt;= freeTime=%v, not eligible for garbage collection\", image.id, image.lastUsed, freeTime) continue &#125; // Avoid garbage collect the image if the image is not old enough. // In such a case, the image may have just been pulled down, and will be used by a container right away. if freeTime.Sub(image.firstDetected) &lt; im.policy.MinAge &#123; klog.V(5).Infof(\"Image ID %s has age %v which is less than the policy's minAge of %v, not eligible for garbage collection\", image.id, freeTime.Sub(image.firstDetected), im.policy.MinAge) continue &#125; // Remove image. Continue despite errors. klog.Infof(\"[imageGCManager]: Removing image %q to free %d bytes\", image.id, image.size) err := im.runtime.RemoveImage(container.ImageSpec&#123;Image: image.id&#125;) if err != nil &#123; deletionErrors = append(deletionErrors, err) continue &#125; delete(im.imageRecords, image.id) spaceFreed += image.size if spaceFreed &gt;= bytesToFree &#123; break &#125; &#125; if len(deletionErrors) &gt; 0 &#123; return spaceFreed, fmt.Errorf(\"wanted to free %d bytes, but freed %d bytes space with errors in image deletion: %v\", bytesToFree, spaceFreed, errors.NewAggregate(deletionErrors)) &#125; return spaceFreed, nil&#125; 磁盘驱逐与 ImageGC除了上述 GC 逻辑外，实际上还有额外的 ImageGC 触发条件。在运行中，偶尔会遇到 ImageGCHighThresholdPercent 被设置为 100 但还是有镜像被清理的情况。我们反过来看下在上文提到的 ImageGC 的接口，可以看到 DeleteUnusedImages 是个 public 方法。 1234567891011121314151617181920func buildSignalToNodeReclaimFuncs(imageGC ImageGC, containerGC ContainerGC, withImageFs bool) map[evictionapi.Signal]nodeReclaimFuncs &#123; signalToReclaimFunc := map[evictionapi.Signal]nodeReclaimFuncs&#123;&#125; // usage of an imagefs is optional if withImageFs &#123; // with an imagefs, nodefs pressure should just delete logs signalToReclaimFunc[evictionapi.SignalNodeFsAvailable] = nodeReclaimFuncs&#123;&#125; signalToReclaimFunc[evictionapi.SignalNodeFsInodesFree] = nodeReclaimFuncs&#123;&#125; // with an imagefs, imagefs pressure should delete unused images signalToReclaimFunc[evictionapi.SignalImageFsAvailable] = nodeReclaimFuncs&#123;containerGC.DeleteAllUnusedContainers, imageGC.DeleteUnusedImages&#125; signalToReclaimFunc[evictionapi.SignalImageFsInodesFree] = nodeReclaimFuncs&#123;containerGC.DeleteAllUnusedContainers, imageGC.DeleteUnusedImages&#125; &#125; else &#123; // without an imagefs, nodefs pressure should delete logs, and unused images // since imagefs and nodefs share a common device, they share common reclaim functions signalToReclaimFunc[evictionapi.SignalNodeFsAvailable] = nodeReclaimFuncs&#123;containerGC.DeleteAllUnusedContainers, imageGC.DeleteUnusedImages&#125; signalToReclaimFunc[evictionapi.SignalNodeFsInodesFree] = nodeReclaimFuncs&#123;containerGC.DeleteAllUnusedContainers, imageGC.DeleteUnusedImages&#125; signalToReclaimFunc[evictionapi.SignalImageFsAvailable] = nodeReclaimFuncs&#123;containerGC.DeleteAllUnusedContainers, imageGC.DeleteUnusedImages&#125; signalToReclaimFunc[evictionapi.SignalImageFsInodesFree] = nodeReclaimFuncs&#123;containerGC.DeleteAllUnusedContainers, imageGC.DeleteUnusedImages&#125; &#125; return signalToReclaimFunc&#125; 实际上，在磁盘满导致节点驱逐信号触发时会直接调用容器和镜像的 GC 方法，毕竟节点驱逐的触发是更紧急的。 总结总的来看，Kubelet 会在节点驱逐信号触发和Image 对应的 Filesystem 空间不足的情况下删除冗余的镜像。整个 GC 的要点如下： 清理的触发为到达 HighThresholdPercent 开始清理，一直清理到 LowThresholdPercent 为止。但是需要注意的是通过将 HighThresholdPercent 设置为 100 关闭 GC 的做法对节点驱逐不生效，只能关闭定时清理任务 镜像清理过程中，有三类镜像不会被清除： Sanbox 所需镜像； GC 首次探测和刚被刷新过最后使用时间的镜像； 探测累计时长小于 MinimumGCAge 的镜像。 清理过程会优先清除最久没用到的和最早探测到的镜像。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"}]},{"title":"博客写作的工作流","slug":"博客写作的工作流","date":"2021-01-02T15:47:00.000Z","updated":"2022-05-21T07:33:16.904Z","comments":true,"path":"2021-01-02-blogs-workflow/","link":"","permalink":"https://study4.fun/2021-01-02-blogs-workflow/","excerpt":"","text":"引言一开始就把博客写作叫成一个工作流，是因为我们很多时候在写博客的时候，懒惰的很大一部分原因是因为觉得麻烦。当你在使用 hexo 这种静态博客的时候，难免会觉得写作是一件很繁琐的时间，首先会受限于写作环境，其次每次写作都需要“写 md –&gt; 插入图片上传 –&gt; 预览后编译 –&gt; 推送到 pages”。因为工作的原因，用了很久的语雀，感觉语雀的写作体验和管理都是很棒的。那么，如何拥有语雀写作的体验，又能够免去博客更新的繁碎流程呢？ 关于工作流实现配置的文章网上一堆，我就不当个搬运工了，说下整个流程吧： 在语雀的指定仓库写文章，勾选发布的“文档有较大更新，推送给关注知识库的人” 你在该仓库下设置的 WebHook 触发云平台上的函数服务 函数服务发送对应的请求给 Github 的 repo dispatch dispatch 触发 Github Action，开始构建网页并推送到目标平台（Pages/COS/OSS)，推送完毕后自动刷新 CDN 完成后，获得的是这样的一套写作平台体验： 复用了语雀优秀的文档写作和管理体验，做到 Write Every Where 除了域名外，几乎全免费 省去了博客的维护成本（Serverless？） 当然了，初次配置的成本也是不低的。不过对于程序员来说，哪怕首次配置麻烦，后面能省掉很多时间成本也是一件很赚的事情吧。（何况语雀比本地 markdown 好用多了）而且每次写作不用限制在某一台电脑上。 一些细节 语雀的 TOKEN 权限设置为只读就 OK 可以使用语雀作为图床白嫖一下，需要额外配置一下语雀的防盗链 国内的收录需要博客在国内可访问，使用 Pages+CDN 可以达到效果，又拍云免费 https 证书操作比较麻烦，可以用第三方网站签发免费的泛域名证书，只是记得定时要在 CDN 上去配置 刷新 CDN 的 API 调用记得配置全站路径 参考语雀知识库同步工具Hexo：语雀云端写作，Github Actions 持续集成","categories":[{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/categories/%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/tags/%E6%9D%82%E9%A1%B9/"}]},{"title":"CGroup初探","slug":"CGroup初探","date":"2020-10-30T12:55:00.000Z","updated":"2022-05-21T07:33:16.844Z","comments":true,"path":"2020-10-30-cgroup-startings/","link":"","permalink":"https://study4.fun/2020-10-30-cgroup-startings/","excerpt":"","text":"基本概念namespace：隔离进程组之间的资源cgroup：对一组进程进行统一的资源监控和限制，进程按组进行管理的机制cgroup 的构成： subsystem：对应内核模块，用于对进程组进行操作。 hierarchy：一棵 cgroup 树，用于进程分组。 其他： 在任意一个 hierarchy 内包含 linux 系统的所有 process。 在同一个 hierarchy 内，process 属于唯一的 process group。但 process 可以存在于不同的 hierarchy 中。 hierarchy 会对应若干不同的 subsystem（可以为 0），一个 subsystem 只能关联一个 hierarchy。systemd 没有对应的 subsystem。 process -&gt; process group（cgroup） -&gt; hierarchy树节点 -&gt; 树 Subsystemsubsystem namesubsystem 关联 hierarchy id：如果为 0，无绑定/cgroup v2 绑定/未被内核开启num of cgroups：关联 hierarchy 内进程组个数enabled：是否开启，通过内核参数 cgroup_disable 调整 详细cpu (since Linux 2.6.24; CONFIG_CGROUP_SCHED)用来限制 cgroup 的 CPU 使用率。 cpuacct (since Linux 2.6.24; CONFIG_CGROUP_CPUACCT)统计 cgroup 的 CPU 的使用率。 cpuset (since Linux 2.6.24; CONFIG_CPUSETS)绑定 cgroup 到指定 CPUs 和 NUMA 节点。 memory (since Linux 2.6.25; CONFIG_MEMCG)统计和限制 cgroup 的内存的使用率，包括 process memory, kernel memory, 和 swap。 devices (since Linux 2.6.26; CONFIG_CGROUP_DEVICE)限制 cgroup 创建(mknod)和访问设备的权限。 freezer (since Linux 2.6.28; CONFIG_CGROUP_FREEZER)suspend 和 restore 一个 cgroup 中的所有进程。 net_cls (since Linux 2.6.29; CONFIG_CGROUP_NET_CLASSID)将一个 cgroup 中进程创建的所有网络包加上一个 classid 标记，用于 tc 和 iptables。 只对发出去的网络包生效，对收到的网络包不起作用。 blkio (since Linux 2.6.33; CONFIG_BLK_CGROUP)限制 cgroup 访问块设备的 IO 速度。 perf_event (since Linux 2.6.39; CONFIG_CGROUP_PERF)对 cgroup 进行性能监控 net_prio (since Linux 3.3; CONFIG_CGROUP_NET_PRIO)针对每个网络接口设置 cgroup 的访问优先级。 hugetlb (since Linux 3.5; CONFIG_CGROUP_HUGETLB)限制 cgroup 的 huge pages 的使用量。 pids (since Linux 4.3; CONFIG_CGROUP_PIDS)限制一个 cgroup 及其子孙 cgroup 中的总进程数。 使用12345678910# 挂载一颗和cpuset subsystem关联的cgroup树到&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpusetmkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpusetmount -t cgroup -o cpuset xxx &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpuset# 不关联任何子系统mkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;systemdmount -t cgroup -o none,name&#x3D;systemd xxx &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;systemd# 关联所有子系统mount -t cgroup xxx &#x2F;sys&#x2F;fs&#x2F;cgroup 查看进程对应的 cgroupproc/[pid]/cgrouphierarchy id : subsystems : 进程在 hierarchy 中的相对路径 创建管理 cgroupcgroup 目录 cgroup.clone_children这个文件只对 cpuset subsystem 有影响，当该文件的内容为 1 时，新创建的 cgroup 将会继承父 cgroup 的配置，即从父 cgroup 里面拷贝配置文件来初始化新 cgroup，可以参考这里 cgroup.procs当前 cgroup 中的所有进程 ID，系统不保证 ID 是顺序排列的，且 ID 有可能重复 cgroup.sane_behavior具体功能不详，可以参考这里和这里 notify_on_release该文件的内容为 1 时，当 cgroup 退出时（不再包含任何进程和子 cgroup），将调用 release_agent 里面配置的命令。新 cgroup 被创建时将默认继承父 cgroup 的这项配置。 release_agent里面包含了 cgroup 退出（移出）时将会执行的命令，系统调用该命令时会将相应 cgroup 的相对路径当作参数传进去。 注意：这个文件只会存在于 root cgroup 下面，其他 cgroup 里面不会有这个文件。 tasks当前 cgroup 中的所有线程 ID，系统不保证 ID 是顺序排列的 创建子 cgroup新建子文件夹 添加进程 在一颗 cgroup 树里面，一个进程必须要属于一个 cgroup。 新创建的子进程将会自动加入父进程所在的 cgroup 从一个 cgroup 移动一个进程到另一个 cgroup 时，只要有目的 cgroup 的写入权限就可以了，系统不会检查源 cgroup 里的权限。 用户只能操作属于自己的进程，不能操作其他用户的进程，root 账号除外 打印当前 shell pidecho $$ 1sh -c &#39;echo 1421 &gt; ..&#x2F;cgroup.procs&#39; 将 pid 写入，即是进程加入 cgroup。pid 不可在文件中删除，只可以被转移。因为 在一颗 cgroup 树里面，一个进程必须要属于一个 cgroup Pid Subsystempids.current: 表示当前 cgroup 及其所有子孙 cgroup 中现有的总的进程数量pids.max: 当前 cgroup 及其所有子孙 cgroup 中所允许创建的总的最大进程数量通过写入 pids.max 限制成功 子孙 cgroup 中的 pids.max 大小不能超过父 cgroup 中的大小 子 cgroup 中的进程数不仅受自己的 pids.max 的限制，还受祖先 cgroup 的限制 pids.current &gt; pids.max 出现的情况： 设置 pids.max 时，将其值设置的已经比 pids.current 小 pids.max 只会在当前 cgroup 中的进程 fork、clone 的时候生效，将其他进程加入到当前 cgroup 时，不会检测 pids.max Memory Subsystem 1234567891011121314cgroup.event_control #用于eventfd的接口 memory.usage_in_bytes #显示当前已用的内存 memory.limit_in_bytes #设置&#x2F;显示当前限制的内存额度 memory.failcnt #显示内存使用量达到限制值的次数 memory.max_usage_in_bytes #历史内存最大使用量 memory.soft_limit_in_bytes #设置&#x2F;显示当前限制的内存软额度 memory.stat #显示当前cgroup的内存使用情况 memory.use_hierarchy #设置&#x2F;显示是否将子cgroup的内存使用情况统计到当前cgroup里面 memory.force_empty #触发系统立即尽可能的回收当前cgroup中可以回收的内存 memory.pressure_level #设置内存压力的通知事件，配合cgroup.event_control一起使用 memory.swappiness #设置和显示当前的swappiness memory.move_charge_at_immigrate #设置当进程移动到其他cgroup中时，它所占用的内存是否也随着移动过去 memory.oom_control #设置&#x2F;显示oom controls相关的配置 memory.numa_stat #显示numa相关的内存 设置了内存限制立即生效 –&gt; 物理内存使用量达到 limit –&gt; memory.failcnt +1 –&gt; 内核会尽量将物理内存中的数据移到 swap 空间上去 –&gt; 设置的 limit 过小，或者 swap 空间不足 –&gt; kill 掉 cgroup 内继续申请内存的进程（默认行为）详细链接 ## CPU Subsystemcfs_period_us：时间周期长度cfs_quota_us：在一个周期长度内所能使用的 CPU 时间数cpu.shares： 针对所有的 CPU 的相对值，默认值是 1024 仅在 CPU 忙时起作用 无法精确的控制 CPU 使用率，因为 cgroup 会动态变化 limit_in_cores = cfs_period_us / cfs_quota_us Refhttps://segmentfault.com/a/1190000006917884","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"容器","slug":"容器","permalink":"https://study4.fun/tags/%E5%AE%B9%E5%99%A8/"},{"name":"cgroup","slug":"cgroup","permalink":"https://study4.fun/tags/cgroup/"}]},{"title":"QoS in Kubernetes","slug":"QoS in Kubernetes","date":"2020-10-26T05:55:00.000Z","updated":"2022-05-21T07:33:16.884Z","comments":true,"path":"2020-10-26-qos-in-kubernetes/","link":"","permalink":"https://study4.fun/2020-10-26-qos-in-kubernetes/","excerpt":"","text":"1.原则 通过 limit 保证不同 pod 只能占用指定的资源 集群资源允许被超额分配 为 pod 划分等级确保不同的 QoS，资源不足时，低等级的会被清理 2.语义limits系统允许容器运行时可能使用的资源量的最高上限最多允许使用的上限，超过时进程会被杀掉 requestsK8S 调度时能为容器提供的完全可保障的资源量最少的资源下限，当 node 上资源少于该值，pod 将不会被调度到此 node 上 m=milli unit，表示千分之一M=1000 3.资源范围cpu/mem/gpu/huge-page(v1.14) 123resources: limits: hugepages-2Mi: 100Mi 4.基于 request 和 limit 的调度机制调度时不看实际的使用资源量，看已运行 pod 的 request 总和作为已占用资源的度量 K8S pod 资源的特点分为 完全可靠资源 和 不可靠资源，通过这种机制实现 超卖完全可靠的资源 = request不可靠资源 = limit - request 可压缩/不可压缩资源可压缩资源：CPU空闲资源按照 Request 的比例进行分配pod 的 cpu 使用超过 limit 时，cgroups 会对 pod 进行限流 throttled 不可压缩资源：内存超过 request 可能会被杀掉超过 limit 时，内核会杀掉容器中使用内存最多的一个，直到不超过 limnit 为止 5. 服务质量等级 QoS Class优先级递减：Guaranteed &gt; Burstable &gt; BestEffort Guaranteed所有容器 request=limit（仅设置 Limit 时也等效） Burstablerequests 不等于 limits BestEffort所有容器的 request 和 limit 都未定义 OOM ScoreOOM Score = 内存占用百分比 * 10 + 调整分（OOM_SCORE_ADJ) OOM_SCORE_ADJGuaranteed: -998, BestEffort: 1000,Busrtable: request &gt; 99.8%内存, 2 &lt;，1000 - 内存占用百分比 * 10 request=0, 999 特殊的 OOM_SCOREkubelet/docker: -998不会被杀掉的进程： -999","categories":[{"name":"备忘录","slug":"备忘录","permalink":"https://study4.fun/categories/%E5%A4%87%E5%BF%98%E5%BD%95/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"}]},{"title":"APUE读书笔记（一）","slug":"APUE读书笔记（一）","date":"2020-08-15T15:55:00.000Z","updated":"2022-05-21T07:33:16.936Z","comments":true,"path":"2020-08-15-apue-readling-notes-1/","link":"","permalink":"https://study4.fun/2020-08-15-apue-readling-notes-1/","excerpt":"","text":"一. 基础知识1.1 体系结构系统调用：内核的接口应用程序既可以使用共用函数库，也可以使用系统调用 1.2 登录登录项的组成：登录名、加密口令、数字用户 ID（UID）、数字组 ID（GID）、注释字段、起始目录、shell 程序 1.3 目录每个进程都有一个工作目录（当前工作目录），相对路径都从工作目录开始解析进程可以使用 chdir 函数更改工作目录 1.4 输入输出1.4.1 文件描述符文件描述符是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表取值范围为 0 到 OPEN_MAX（每个进程最多可以打开 的文件数-1，63） 1.4.2 标准输入、标准输出和标准错误标准输入、标准输出和标准错误是在运行新程序时，shell 为程序默认打开的 3 个文件描述符 1.5 程序和进程内核使用 exec 函数将程序读入内存并执行程序fork 函数创建一个新进程，该进程（子进程）是调用进程（父进程）的一个副本在创建时，对父进程返回子进程 ID，对子进程返回 0 1.6 线程一个进程内的所有线程共享同一地址空间、文件描述符、栈以及与进程相关的属性 1.7 出错处理errno：Unix 系统函数出错时返回的负值关于 errno： 如果没有出错，其值都不会被例程清除 任何函数都不会将 errno 设置为 0 关于错误： 出错分为致命性的和非致命性的 对于致命性错误，可打印出错消息或者写入日志 对于非致命错误，可以尝试重试（延时、指数补偿算法） 1.8 用户标识1. 用户 ID用户不可更改其用户 ID用户 ID 为 0 的用户为 root 用户或超级用户 2. 组 ID组文件将组名映射为数字的组 ID，位于/etc/group 3. 附属组 ID系统允许用户属于另外一些组，一个用户属于多至 16 个其他的组 1.9 信号信号用于通知进程发生了某种状况处理信号的方式： 忽略 系统默认方式处理 提供函数，信号发生时调用，即捕捉信号 1.10 时间日历时间：自 UTC 时间的秒数累计值进程时间：也叫做 CPU 时间，用来度量进程使用的 CPU 资源，以时钟 tick 计算系统为进程维护的三个进程时间值： 时钟时间，进程运行的时间总量（与系统同时运行的进程数有关） 用户 CPU 时间，执行用户指令所用的时间量 系统 CPU 时间，为该进程执行内核程序经历的时间量 用户 CPU 时间和系统 CPU 时间常称为 CPU 时间 12# 度量执行时间time -p &lt;command&gt; 1.11 系统调用和库函数系统调用：提供给程序向内核请求服务的，操作系统提供的入口点区别： 库函数可以被替换，而系统调用通常不可替换 应用程序既可以调用 系统调用，也可以调用 库函数 许多库函数会调用系统调用 系统调用通常是提供最接口，而库函数通常提供较复杂的功能 进程控制系统调用（fork/exec/wait）通常有用户应用程序直接调用","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"Unix","slug":"Unix","permalink":"https://study4.fun/tags/Unix/"}]},{"title":"《代码整洁之道》笔记","slug":"《代码整洁之道》笔记","date":"2020-05-25T02:08:00.000Z","updated":"2022-05-21T07:33:17.032Z","comments":true,"path":"2020-05-25-clean-coder-notes/","link":"","permalink":"https://study4.fun/2020-05-25-clean-coder-notes/","excerpt":"","text":"1. 专业主义1.1 担当责任1.2 不行损害之事（1）不要破坏软件功能，让失误率无限接近于 0 让测试人员找不出问题 不发布无把握的代码 反思 bug 怎么越过测试 确信代码正常运行（自动化测试、单元测试） 自动化 QA （2）不要破坏结构 不为了发布新功能破坏代码的结构 软件要易于修改（需要使用测试保证代码可经常修改） 1.3 职业道德职业发展需要保证每周有自己的时间 领域知识（设计模式、设计原则、方法、实践、工件） 坚持学习（文章、博客、技术大会） 练习（每天一到两道题） 合作 辅导 了解业务领域（了解基础架构、基本知识、原则和价值观念） 站在用户的角度思考 谦逊 2. 说“不” 能就是能，不能就是不能。不要说“试试看”。 作为专业人士，就不应该什么事都照做 2.1 对抗角色面对艰难的决定，直面不同角色的冲突是最好的办法——找到可能的最好结果 “为什么”其实并没有那么重要 2.2 高风险时刻在高风险时刻说不，是对大家负责 2.3 团队精神 不说谎，做出合理预期 如果是“试试看”，那就代表之前留有余力。本质上是在说谎，可能是为了护住面子和避免冲突 不消极对抗，不能任由事态发展 2.4 如何写出好代码坚守专业主义精神，说不 3. 说“是”3.1 承诺用语的三个阶段 口头上说。缺乏承诺，通常是“需要、但愿、让我们”等 心里认真。有清晰的事实陈述，明确说明了期限 真正付诸行动。 3.2 如何做到“言必信，行必果” 如果目标依赖于他人，就应该采取具体行动，推动最终目标 即使感觉目标无法完成，也要弄清楚目标是否能够达成 如果做不到，就应尽早向承诺对象发起预警 3.3 如何说“是”不说“试试”，直接说出所有的可能性 坚守原则 4. 编码4.1 编码需要考虑的问题 代码正常工作 解决客户提出的问题——不是需求，需求不一定能解决问题 和现有的系统能很好的结合 其他程序员可以读懂你的代码 不要在疲劳或者焦虑的时候编码 4.2 流态区这种状态下，感觉效率极高、绝无错误 避免进入流态区，因为这样会进入“放弃顾及全局”的陷阱 4.3 思路阻塞 找个搭档结对编程 创造性输入（PPT、音乐、电影等） 4.4 调试调试和编码一样重要 TDD 可以减少调试时间 4.5 保持节奏在疲劳的时候应该离开工作 回家路上把自己从工作中抽离出来 洗澡时或许会浮现解决方案 4.6 关于进度延迟 不盲目期望提前完成 不盲目冲刺（可以考虑所有情况，缩减交付的范围） 加班（需要保证有个人有时间、短期加班、有加班失败的后备预案） 交付失误时不自欺欺人 定义一个确切的“完成”标准（自动化验收测试） 4.7 帮助他人 帮助他人的时候不应付 学会接受他人的帮助，学会请求帮助 辅导与寻求辅导 5. 测试驱动开发三项法则 在写好失败单元测试前，不要写任何产品代码； 一旦单元测试失败了，就不要写测试代码； 产品代码只需要恰好能够让当前失败的单元测试通过即可，不要多写。 优势 确定性；对代码的掌控程度高。 缺陷注入率；降低代码的 bug 和风险。 勇气；测试完备，修改或者重构的风险降低。 文档；单元测试即示例，清晰准确。 设计；迫使自己考虑好的设计。 6. 练习6.1 练习方式kata练习解决这个问题所需要的动作和决策 学习热键和导航操作、测试驱动开的、持续集成之类的方法 wasa两个人的 kata，一个人写单元测试，另一个人写程序 自由练习多人参与的 wasa 6.2 拓展经验的方法开源 使用自己的时间练习 7.验收测试7.1 需求的沟通不要过早精细化需求 不确定原则（观察者效应）：观察到的新信息会影响对整个系统的看法 预估焦虑：需求一定会变化，追求早期的精准性是徒劳的 迟来的模糊性：需求文档中的模糊之处，都对应着业务方的分歧 7.2 验收测试“完成”的含义 所有的代码写完了 测试通过了 QA 和需求方已经认可 沟通：开发方、业务方、测试方达成共识 自动化：缩减成本 测试并不是额外工作 验收测试与开发应当不是同一个人编写 开发人员的角色是把验收测试和开发系统联系起来保证测试的通过 不能被动接收测试，需要协商并改进 验收测试和单元测试：内部 vs 外部 图形界面和其他复杂因素：调用 API 而不是 GUI 持续集成：失败应立刻终止 8. 测试策略自动化测试金字塔： 单元测试：在最低层次上定义系统。 组件测试：对业务规则的验收测试。 集成测试：装备测试，确认组件之间正确连接，彼此通信畅通。 系统测试：测试系统是否正确组装完毕，以及各个组件之间是否正确交互，同时包括吞吐量测试和性能测试。 人工探索式测试：确保系统在人工才操作表现良好。 9. 时间管理9.1 会议学会拒绝/离席，确定议程和目标，立会，迭代计划会议，争论与反对 凡是不能在五分钟内解决的争论，都不能靠辩论解决。唯一的出路是：用数据说话。可以做试验、模仿或者建模。 9.2 注意力点数学会安排时间，妥善使用自己的注意力点数 睡眠/咖啡/恢复/肌肉注意力/输入和输出 9.3 时间拆分和番茄工作法记录下来并画图展示 9.4 要避免的行为优先级错乱——提高某个任务的优先级，有借口推迟真正急迫的任务 9.5 死胡同不执拗于不容放弃也无法绕开的主意，听取其他意见。越是坚持，浪费的时间越多。 9.6 泥潭走回头路是最简单的办法。 发现身处泥潭还固执前进，是最严重的优先级错乱。 10. 预估10.1 什么是预估承诺还是猜测？ 预估是一种猜测，不包含任何承诺的色彩。不是一个定数，是一种概率分布。 小心给出暗示性的承诺。 10.2 PERT（计划评审技术）乐观预估（1%），标称预估（概率最大），悲观预估（1%） 得出任务的期望完成时间和任务的概率分布标准差（不确定性） 10.3 预估任务德尔菲法： 亮手指：预估相近达到统一，有分歧则讨论 规划扑克 关联预估：多人对任务进行所需时间长短的排序，讨论分歧，任务归类 三元预估：使用德尔菲法分别进行乐观、标称、悲观预估 10.4 大数定律控制错误的方法——把大任务分成小任务，分开预估在加总，结果会比单独评估大任务要准确的多 11. 压力11.1 避免压力避免对没有把握能够达成的最后期限做出承诺 让系统、代码和设计尽可能整洁 在危机时也要遵守纪律原则（例如 TDD） 11.2 应对压力不要惊慌失措：深思熟虑，努力寻找可以带来最好结果的方法 沟通：告诉你制定的走出困境的计划，请求支援和指引 坚信纪律和原则 寻求帮助：结对编程 12. 协作首要职责是满足雇主的需求，和团队协作，深刻理解业务目标，了解企业如何从你的工作中获得回报 拥有代码的是整个团队，而不是个人 专业人士会共同工作，彼此面对面。 13. 团队和项目形成团队需要时间。建立关系，学会互相协作，了解彼此的癖好和长短处，最后凝聚成为团队。 把项目分配给已经形成凝聚力的团队，而不是围绕项目组建团队。 14. 辅导、实习生和技艺辅导： 通过书籍手册向作者学习 通过观察他人工作来学习 实习生： 技术方面你的传授、培训、督导和检查 技艺： 技艺的模因包含价值观、原则、技术、态度和正见 技艺的模因经由口口相传和手手相承而来","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}]},{"title":"2019的闲言碎语","slug":"2019的闲言碎语","date":"2020-01-01T08:45:58.000Z","updated":"2022-05-21T07:33:17.296Z","comments":true,"path":"2020-01-01-2019-comments/","link":"","permalink":"https://study4.fun/2020-01-01-2019-comments/","excerpt":"","text":"回顾总是有种从时间线开始的惯性。2019 年开始于天津不好吃的本地餐厅，结束于一顿火烧云不好吃的外卖。如同不好吃的饭，其实这一年经历的过程也算不上顺遂心意。 平行的工作线年前的两个月忙着做实验室的项目，年后开始准备实习的事情。我常说自己足够幸运，在应该做某件事的时候就去做了这件事，这样尽力就好。但这也只是在事后对于自己的些微自嘲而已，我还是改不掉过分焦虑的坏毛病。正如高考前失眠，考研前默背政治到半夜，准备实习的我还是经常会自己一个人默默焦虑地胡思乱想到失眠。 中间经历了其实不算多的面试，因为想要投的岗位对口的其实并不算多。在实验室的经历让我走了一条和别的同学不太一样的岗位，得益于平时的兴趣比较杂比较宽泛，算是很幸运的拿到了实习岗位。其实回顾来看，最后的结果其实是超乎于我自己意料之外的，因为入职后见过身边远远优秀与我的同学太多。 如果对当前的选择感到迷茫，不如找一个能看清方向的人交流，远比自己尝试碰壁或者和同类人无效讨论要有效得多。希望自己能在以后的经历中谨记住这一点。 工作了三四个月，回顾下其实觉得收益到最多的反而是工作方式。很多时候，我们抱怨工作枯燥，内容无趣。但总是会有人能从这些枯燥的工作中提取出更有效的东西，提高自己的效率，提高自己的思考深度，这让我真切体会到平凡和优秀的区别。说和做，差的总是很远。 后面回到学校准备毕设中期，除了完成一篇自己之前觉得有点难度的水论文，倒也没有什么大的波澜。 平行的生活线对于个人生活来说，19 年真的是很重要的一年。不同于工作上能够絮絮叨叨说出个所以然，对于我来说，生活是由一个个或难忘或开心或感动，也或是平凡幸福的时刻组成的。很多个这样的时刻会慢慢沉淀成一种潜移默化的东西，支撑自己在各种困难的时候坚持过去。今年这样的时刻格外的多，或许因为不再是一个人的原因吧。 如果说要给自己的 2020 寄托一些什么希冀一些什么，大概还是一些意识到却没有做到的东西。 能够想到就去做，但是还没坚持做下去。 能够开始反思，但是还没及时改变。 其实如果能做到这两点，总感觉 2020 就会好。但是人总是有点惰性的，我也还是那个对自己有限悲观的人。接下来的一年，能做到一个，也足够让我满足了。毕竟，顺遂心意永远是最重要的。拧巴地生活，还不如维持现状呢。","categories":[{"name":"日记","slug":"日记","permalink":"https://study4.fun/categories/%E6%97%A5%E8%AE%B0/"}],"tags":[{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/tags/%E6%9D%82%E9%A1%B9/"}]},{"title":"Kubernetes学习笔记——DesiredStateOfWorldPopulator源码分析","slug":"Kubernetes学习笔记——DesiredStateOfWorldPopulator源码分析","date":"2019-09-03T11:20:00.000Z","updated":"2022-05-21T07:33:17.064Z","comments":true,"path":"2019-09-03-desired-state-of-world-populator/","link":"","permalink":"https://study4.fun/2019-09-03-desired-state-of-world-populator/","excerpt":"","text":"我们知道，kubelet 启动时会运行 VolumeManager 协程来负责 Volume 变更时的操作。它主要通过 ActualStateOfWorld 和 DesiredStateOfWorld 这两个 cache 信息来让 VolumeManager 中的两个协程工作。 DesiredStateOfWorldPopulator 和 Reconciler 两个 Goroutine 会通过图中两个的 StateOfWorld 状态进行通信，DesiredStateOfWorldPopulator 主要负责从 Kubernetes 节点中获取新的 Pod 对象并更新 DesiredStateOfWorld 结构；而后者会根据实际状态和当前状态的区别对当前节点的状态进行迁移，也就是通过 DesiredStateOfWorld 中状态的变更更新 ActualStateOfWorld 中的内容。Ref: https://draveness.me/kubernetes-volume 下面就来分析一下 DesiredStateOfWorldPopulator 的源码。 0x02. 结构与接口DesiredStateOfWorldPopulator 的数据结构如下： 123456789101112131415type desiredStateOfWorldPopulator struct &#123; kubeClient clientset.Interface loopSleepDuration time.Duration getPodStatusRetryDuration time.Duration podManager pod.Manager podStatusProvider status.PodStatusProvider desiredStateOfWorld cache.DesiredStateOfWorld actualStateOfWorld cache.ActualStateOfWorld pods processedPods kubeContainerRuntime kubecontainer.Runtime timeOfLastGetPodStatus time.Time keepTerminatedPodVolumes bool hasAddedPods bool hasAddedPodsLock sync.RWMutex&#125; kubeClient：用以从 API Server 获取 PV 和 PVC 对象 loopSleepDuration：定义连续执行的间隔 podManager：host 真实存在的 Pod 信息获取来源 DesiredStateOfWorldPopulator 的接口有三个方法： 1234567type DesiredStateOfWorldPopulator interface &#123; Run(sourcesReady config.SourcesReady, stopCh &lt;-chan struct&#123;&#125;) ReprocessPod(podName volumetypes.UniquePodName) HasAddedPods() bool&#125; 除了核心执行方法 Run，ReprocessPod 能够将特定 Pod 强制剔出 processedPods 列表进行强制重新处理。该方法用于在 Pod 更新上启用重新挂载卷。而 HasAddedPods 方法则返回 populator 是否已经将所有现有 Pod 处理添加到 desired state 中。 0x03. 核心流程分析3.1 populatorLoopFuncrun 方法中，每隔 loopSleepDuration 就会执行一次 populatorLoopFunc。 1234567891011121314151617func (dswp *desiredStateOfWorldPopulator) populatorLoopFunc() func() &#123; return func() &#123; dswp.findAndAddNewPods() if time.Since(dswp.timeOfLastGetPodStatus) &lt; dswp.getPodStatusRetryDuration &#123; glog.V(5).Infof( \"Skipping findAndRemoveDeletedPods(). Not permitted until %v (getPodStatusRetryDuration %v).\", dswp.timeOfLastGetPodStatus.Add(dswp.getPodStatusRetryDuration), dswp.getPodStatusRetryDuration) return &#125; dswp.findAndRemoveDeletedPods() &#125;&#125; 3.2 findAndAddNewPodsfindAndAddNewPods 遍历所有 Pod 并且将“应该添加到期望状态但实际上没有添加”的 Pod 添加到对应状态值中。 分析流程可知该方法先寻找不是终止状态的 Pod，再调用 processPodVolumes 处理这些符合条件的 Pod。 123456789func (dswp *desiredStateOfWorldPopulator) findAndAddNewPods() &#123; for _, pod := range dswp.podManager.GetPods() &#123; if dswp.isPodTerminated(pod) &#123; // Do not (re)add volumes for terminated pods continue &#125; dswp.processPodVolumes(pod) &#125;&#125; 终止状态判定生效条件满足一条即判定为终止状态： Phase 处于 PodFailed Phase 处于 Succeeded 删除时间不为空，且所有内部容器状态 ContainerStatus 都为 Terminated 或者 Waiting，或者 Container List 为空。 终止状态判定完毕，核心方法 processPodVolumes 会将给定 Pod 中的 Volumes 进行处理并添加到期望状态值中。 1234567891011121314151617181920212223242526272829303132// processPodVolumes processes the volumes in the given pod and adds them to the desired state of the world.func (dswp *desiredStateOfWorldPopulator) processPodVolumes(pod *v1.Pod) &#123; ... uniquePodName := util.GetUniquePodName(pod) if dswp.podPreviouslyProcessed(uniquePodName) &#123; return &#125; allVolumesAdded := true mountsMap, devicesMap := dswp.makeVolumeMap(pod.Spec.Containers) // Process volume spec for each volume defined in pod for _, podVolume := range pod.Spec.Volumes &#123; volumeSpec, volumeGidValue, err := dswp.createVolumeSpec(podVolume, pod.Name, pod.Namespace, mountsMap, devicesMap) .... // Add volume to desired state of world _, err = dswp.desiredStateOfWorld.AddPodToVolume( uniquePodName, pod, volumeSpec, podVolume.Name, volumeGidValue) .... &#125; if allVolumesAdded &#123; dswp.markPodProcessed(uniquePodName) // New pod has been synced. Re-mount all volumes that need it dswp.actualStateOfWorld.MarkRemountRequired(uniquePodName) &#125;&#125; 可以看到，该方法处理流程如下： 判断该 Pod 之前是否被处理过，处理过则返回。即从 dswp 的 processedPods 列表中查找，需要加读锁。 通过 Container 列表创建 mountsMap 和 devicesMap，mountsMap 存储 VolumeMounts 字段的挂载信息，devicesMap 存储 BlockVolume 的信息。Key=VolumeMount.Name，Value=True。 对 Spec 中的 Volumes 列表中每一项 PodVolume，根据 PodName, Namespace, mountsMap, devicesMap 创建 VolumeSpec。 根据 Pod,PodName,VolumeSpec， PV Name，GID 将 Volume 添加到 dswp 缓存的 desiredStateOfWorld 中。 如果全部 Volume 的添加都成功则将 Pod 标记为“Processed”，同时将 Pod 标记为 RemountRequired 状态用以更新 Volume 的内容。 其中，步骤 3 的 createVolumeSpec 首先会判断该 podVolume 的 Source 是否为 PVC，如果为 PVC 则需要找到 Claim 背后的 PV Name，再通过 PV Name 获取真正的 PV 对象并返回。如果 PVC 为空，则对 PV 深拷贝并创建 Spec 对象返回。 1234567891011121314151617func (dswp *desiredStateOfWorldPopulator) getPVCExtractPV( namespace string, claimName string) (string, types.UID, error) &#123; pvc, err := dswp.kubeClient.CoreV1().PersistentVolumeClaims(namespace).Get(claimName, metav1.GetOptions&#123;&#125;) if err != nil || pvc == nil &#123; return \"\", \"\", fmt.Errorf(......) &#125; ... if pvc.Status.Phase != v1.ClaimBound || pvc.Spec.VolumeName == \"\" &#123; return \"\", \"\", fmt.Errorf(......) &#125; return pvc.Spec.VolumeName, pvc.UID, nil&#125; 实际上，通过 PVC 找 PV Name 是由 KubeClient 向 API Server 请求得到的。请求通过 Namespace 和 Claim Name 获取到 PVC 对象，确认 PVC 对象的 Phase 为 Bound 状态且 pvc.Spec.VolumeName 不为空。上述流程成功后返回 PVC 的 VolumeName（即 PV Name）和该 PVC 的 UID。 获取到 pvName 和 pvcUID 后，再次通过 KubeClient 向 API Server 请求得到 PV 对象。请求成功后检查 ClaimRef 是否为空，ClaimRef 的 UID 和传入的 PVC UID 是否一致。最后返回该 PV 对象，在返回的同时一并返回的还有 PV 的 GID。 再看看步骤 4，其调用的 AddPodToVolume 方法如果检查到没有可用的 Volume 插件或者可用插件不止一个，会返回 Error。如果 Pod Unique Name 重复，则不执行任何操作。此外，如果 Volume Name 如果不在该节点的 Volume 列表中，则该 Volume 会被隐式添加( implicitly added)。 If a volume with the name volumeName does not exist in the list of volumes that should be attached to this node, the volume is implicitly added. 3.3 findAndRemoveDeletedPods先从 desiredStateOfWorld 中遍历待挂载的 Volume，然后从 PodManager 中根据待挂载 Volume 的 Pod UID 查找该对应 Pod。跳过正在运行和不需要删除 Volumes（keepTerminatedPodVolumes）的 Pod，执行删除流程。 当 Pod 从 PodManager 中删除 Pod 时，Pod 不会在 Volume Manager 中立即删除，需要确认 kubelet 容器运行时所有的 Container 已经全部终止。此外，同时还要确认 actualStateOfWorld 缓存中是否存在待挂载 Volume 信息。 上述确认过程确认完毕后，从 desiredStateOfWorld 缓存中删除 Pod，表明指定的 Pod 不再需要该 Volume。同时，从 dswp 维护的 processedPods 列表中删除该 Pod。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"}]},{"title":"《基于容器的分布式系统设计模式》读书笔记","slug":"《基于容器的分布式系统设计模式》读书笔记","date":"2019-08-29T11:27:00.000Z","updated":"2022-05-21T07:33:16.940Z","comments":true,"path":"2019-08-29-dpfcds-notes/","link":"","permalink":"https://study4.fun/2019-08-29-dpfcds-notes/","excerpt":"","text":"设计模式的意义让缺少经验的开发者能够更容易地开发出可复用且高效稳定的程序 分类单容器单节点多节点","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"容器","slug":"容器","permalink":"https://study4.fun/tags/%E5%AE%B9%E5%99%A8/"},{"name":"设计模式","slug":"设计模式","permalink":"https://study4.fun/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"Kubernetes Java Client代码学习——list all pods","slug":"Kubernetes Java Client代码学习——list all pods","date":"2019-05-22T08:08:00.000Z","updated":"2022-05-21T07:33:17.096Z","comments":true,"path":"2019-05-22-list-all-pods/","link":"","permalink":"https://study4.fun/2019-05-22-list-all-pods/","excerpt":"","text":"从官方提供的代码入手，”列出所有 Pod”的示例代码如下： 123456789101112public class Example &#123; public static void main(String[] args) throws IOException, ApiException&#123; ApiClient client = Config.defaultClient(); Configuration.setDefaultApiClient(client); CoreV1Api api = new CoreV1Api(); V1PodList list = api.listPodForAllNamespaces(null, null, null, null, null, null, null, null, null); for (V1Pod item : list.getItems()) &#123; System.out.println(item.getMetadata().getName()); &#125; &#125;&#125; 首先，给出一张手绘的流程图。 0x02. 核心流程2.1 创建 ApiClient 对象首先是创建 ApiClient 对象，从 defaultClient 方法切入，它返回的是一个由 ClientBuilder 的 standard 方法创建的 ApiClient 对象。 123public static ApiClient defaultClient() throws IOException &#123; return ClientBuilder.standard().build();&#125; 2.1.1 Standard 方法继续追踪 ClientBuilder 中 Standard 方法的源码，代码注释显示该方法会通过四种预先配置好的方式中的一种创建一个 builder，优先度顺序如下： 如果环境变量中 KUBECONFIG 定义过，则直接使用该配置； 如果$HOME/.kube/config可以被找到，则使用该配置； 如果In-cluster Service Account能被找到的话，则它就承担集群配置的功能； 上述都不存在，则默认使用localhost：8080作为最后的方法。 如果配置文件或者对应的配置无效的话，会抛出 ConnectException 异常。下来根据代码来验证以上的注释内容： 123456789101112131415161718192021222324252627282930313233public static ClientBuilder standard(boolean persistConfig) throws IOException &#123; #1.首先从环境变量中找ENV_KUBECONFIG final File kubeConfig = findConfigFromEnv(); #找到则读取对应配置文件，执行对应加载过程并返回 if (kubeConfig != null) &#123; try (FileReader kubeConfigReader = new FileReader(kubeConfig)) &#123; KubeConfig kc = KubeConfig.loadKubeConfig(kubeConfigReader); if (persistConfig) &#123; kc.setPersistConfig(new FilePersister(kubeConfig)); &#125; return kubeconfig(kc); &#125; &#125; #2.其次从Home目录找.kube/config文件 final File config = findConfigInHomeDir(); //完成目录文件路径的拼接并返回File对象 if (config != null) &#123; try (FileReader configReader = new FileReader(config)) &#123; KubeConfig kc = KubeConfig.loadKubeConfig(configReader); if (persistConfig) &#123; kc.setPersistConfig(new FilePersister(config)); &#125; return kubeconfig(kc); &#125; &#125; #3.采用集群内ServiceCount方式配置，这里传入的是SC对应的证书文件路径 final File clusterCa = new File(SERVICEACCOUNT_CA_PATH); if (clusterCa.exists()) &#123; return cluster(); &#125; #4.上述方式无效则使用默认构造器方法创建一个实例，实例中DEFAULT_FALLBACK_HOST指定的就是http://localhost:8080 # private String basePath = Config.DEFAULT_FALLBACK_HOST; return new ClientBuilder();&#125; 可以看到代码中实现与注释一一对应，在第一二种配置方式中，核心的方法主要有 KubeConfig 的 loadKubeConfig 和 setPersistConfig 两个方法。从字面上来看，前者主要负责配置加载，而后者则是对于 PersistConfig 的设置，具体是什么后面再看。 首先分析 loadKubeConfig 方法： 123456789101112131415161718192021public static KubeConfig loadKubeConfig(Reader input) &#123; #可以看到这里将输入读取为一个yaml对象， Yaml yaml = new Yaml(new SafeConstructor()); Object config = yaml.load(input); #然后将yaml形式的config对象转化为Map Map&lt;String, Object&gt; configMap = (Map&lt;String, Object&gt;) config; #通过转化为的map取出对应的五个值 String currentContext = (String) configMap.get(\"current-context\"); ArrayList&lt;Object&gt; contexts = (ArrayList&lt;Object&gt;) configMap.get(\"contexts\"); ArrayList&lt;Object&gt; clusters = (ArrayList&lt;Object&gt;) configMap.get(\"clusters\"); ArrayList&lt;Object&gt; users = (ArrayList&lt;Object&gt;) configMap.get(\"users\"); Object preferences = configMap.get(\"preferences\"); #将取出来的值装配到kubeconfig对象里，最后返回 KubeConfig kubeConfig = new KubeConfig(contexts, clusters, users); kubeConfig.setContext(currentContext); kubeConfig.setPreferences(preferences); return kubeConfig;&#125; 从 KubeConfig 类的变量声明能看到一些额外的信息： 123456789101112131415161718// 找到kubeconfig文件的默认地址相关字段public static final String ENV_HOME = \"HOME\";public static final String KUBEDIR = \".kube\";public static final String KUBECONFIG = \"config\";private static Map&lt;String, Authenticator&gt; authenticators = new HashMap&lt;&gt;();// 作者留下的注释//“致读者：我曾考虑过不使用多个Map，而是创建一个config对象并解析，但是使用多个Map要比一堆样板类更加清晰易懂”private ArrayList&lt;Object&gt; clusters;private ArrayList&lt;Object&gt; contexts;private ArrayList&lt;Object&gt; users;String currentContextName;Map&lt;String, Object&gt; currentContext;Map&lt;String, Object&gt; currentCluster;Map&lt;String, Object&gt; currentUser;String currentNamespace;Object preferences;ConfigPersister persister; standard 方法中，在执行完 loadKubeConfig 对象之后，会对传入的 persistConfig 标志位进行判断，如果为 true，则执行 setPersistConfig 方法： 123if (persistConfig) &#123; kc.setPersistConfig(new FilePersister(kubeConfig));&#125; 而 FilePersister 是 ConfigPersister 接口的一个具体实现，该接口仅有一个 save 方法，应当是将配置持久化的方法。 123456789public interface ConfigPersister &#123; public void save( ArrayList&lt;Object&gt; contexts, ArrayList&lt;Object&gt; clusters, ArrayList&lt;Object&gt; users, Object preferences, String currentContext) throws IOException;&#125; standard 方法最后的执行过程是 kubeconfig 方法，方法的注释说明该方法用于从一个预配置好的 KubeConfig 对象中创建 builder，具体源码如下： 123456789101112131415161718192021222324252627public static ClientBuilder kubeconfig(KubeConfig config) throws IOException &#123; final ClientBuilder builder = new ClientBuilder(); #拼装server字段 String server = config.getServer(); if (!server.contains(\"://\")) &#123; if (server.contains(\":443\")) &#123; server = \"https://\" + server; &#125; else &#123; server = \"http://\" + server; &#125; &#125; #根据证书取出KubeConfig的数据或者文件 final byte[] caBytes = KubeConfig.getDataOrFile( config.getCertificateAuthorityData(), config.getCertificateAuthorityFile()); #将kubeconfig对应的数据取出来填充进builder对象中 if (caBytes != null) &#123; builder.setCertificateAuthority(caBytes); &#125; builder.setVerifyingSsl(config.verifySSL()); builder.setBasePath(server); builder.setAuthentication(new KubeconfigAuthentication(config)); return builder;&#125; 2.1.2 Build 方法经历以上过程，终于 standard 方法执行完毕并返回了一个包含各种认证鉴权相关信息的 builder 对象，接下来看看 build 方法的实现。 1234567891011121314151617181920212223public ApiClient build() &#123; final ApiClient client = new ApiClient(); if (basePath != null) &#123; if (basePath.endsWith(\"/\")) &#123; basePath = basePath.substring(0, basePath.length() - 1); &#125; client.setBasePath(basePath); &#125; client.setVerifyingSsl(verifyingSsl); if (authentication != null) &#123; authentication.provide(client); &#125; //上述顺序很重要，因为一旦SSL信息更改，API客户端就会重新评估CA证书。这意味着如果上述流程发生在下面这个调用的后面，在尝试加载证书时，很可能会调用的InputStream已经耗尽。 因此，设置CA证书的顺序必须位于最后。 if (caCertBytes != null) &#123; client.setSslCaCert(new ByteArrayInputStream(caCertBytes)); &#125; return client;&#125; 2.2 剩余工作经历以上步骤，终于完成了 ApiClient 复杂的创建过程。继续看看看看剩余工作，如下： 1234567891011#创建client对象ApiClient client = Config.defaultClient();#将client对象传入到Configuration类的类静态变量中Configuration.setDefaultApiClient(client);#CoreV1Api无参构造方法调用重载的带apiClient参数的构造方法，从Configuration类中取出client对象完成构造CoreV1Api api = new CoreV1Api();#获得CoreV1Api对象后，填充参数执行对应的查询方法V1PodList list = api.listPodForAllNamespaces(null, null, null, null, null, null, null, null, null); 0x03. 总结流程经历以上的分析过程，大致完成了 list pods 的流程分析。结合 K8S 官方的概念流程文档可以验证之前学习的基于 API Server 的安全机制流程，可以看到 Authentication 和 Authorization 部分的数据填充过程。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"}]},{"title":"Go学习笔记——基础知识","slug":"Go学习笔记——基础知识","date":"2019-05-17T09:32:00.000Z","updated":"2022-05-21T07:33:17.168Z","comments":true,"path":"2019-05-17-go-basic/","link":"","permalink":"https://study4.fun/2019-05-17-go-basic/","excerpt":"","text":"特点 静态语言，支持运行时动态类型；支持隐式类型推导； 接口基于Duck 模型；仅通过接口支持多态；不支持泛型但支持反射； 编译成可执行程序直接执行；支持自动垃圾回收；语言原生支持并发；跨平台；多应用于云计算基础设施软件、中间件、区块链等。 程序结构123456789# 文件名：hello.go# 包名，main为可执行程序的包名package main# 引入外部包fmt（标准输入输出）import \"fmt\"# 函数声明，main代表程序入口函数func main()&#123; fmt.Printf(\"Hello, world.\\n\")&#125; 补充： Go 的源程序默认使用 UTF-8 编码 语句结尾的分号可以省略 “{”必须在函数头所在行尾部，不能单独起一行 main 函数所在的包名必须是 main。 编译运行 编译 go build hello.go 运行 ./hello 标识符（仅记录与 Java 不同的） 关键字常量声明 const、变量定义 var、函数定义 func、延迟执行 defer、 结构类型定义 struct、通道类型 chan 数据类型标识 类型 标识 整型 byte int int8-int64 uint uint8-uint64 uintptr （byte 就是 uint8） 浮点数 float32 float64 （自动类型推断为 float64） 复数 complex64 complex128 （由两个 float 构成，对应实部和虚部） 字符 rune 接口 error 连续枚举类型 iota 匿名变量 _ iota 用法: iota 用于常量声明中，初始值为 0，逐行增加 123456const ( a = iota //iota=0,a=0 b //iota=1,b=1 c = 3 //iota=2,未使用，c=3 d = 1 &lt;&lt; iota //iota=3,d=8) 注意：Go 语言里自增和自减是语句而不是表达式[1] 变量和常量变量声明1234567//显式声明，value可以是表达式，不指定则初始化为类型零值，声明后立即分配空间var varName dataType [ = value]//短类型声明，只能出现在函数内，自动进行数据类型推断varName := value//支持多个类型变量同时声明并赋值a, b := 1, \"hello\" 字符串和切片 字符串可以通过类似数组索引的方式访问，但是不能修改 字符串转换为切片[]byte()在数据量大的时候要慎用，因为转换的时候需要复制内容 字符串的底层实现是一个指向字节数组的指针和字节数组长度 基于字符串创建的切片指向原字符串指向的字符数组，不可修改 指针 结构体指针访问结构体字段仍然使用”.”操作符，没有“-&gt;” 不支持指针运算（GO 支持垃圾回收，语言层面禁止指针运算） 允许返回局部变量地址 切片切片（可变数组）维护三个元素——指向底层数组的指正、切片元素数量、底层数组容量 创建方式：数组索引、make MapGo 内置的 map 不是并发安全的，需要时用 sync 包内的 map 保证并发安全 map 键值对的修改不能通过 map 引用直接修改键值，需要 KV 整体赋值 控制结构If-else123if initialization; condition &#123; // do something&#125; Switch 条件表达式支持任意支持相等比较运算的类型变量 switch 后面可以带上初始化语句 case 后可以使用多个值比较，使用逗号分隔 配合使用可以进行类型查询 Goto goto 需要配合标签使用 goto 只能在函数内跳转，但是不能跳过内部变量声明语句，只能跳过同级作用域或上层作用域 Q：【1】这是否意味着自增或自减是原子操作？答：不是。","categories":[{"name":"备忘录","slug":"备忘录","permalink":"https://study4.fun/categories/%E5%A4%87%E5%BF%98%E5%BD%95/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://study4.fun/tags/Golang/"},{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"Kubernetes学习笔记——基于API Server的安全机制浅析","slug":"Kubernetes学习笔记——基于API Server的安全机制浅析","date":"2019-04-25T02:50:00.000Z","updated":"2022-05-21T07:33:17.140Z","comments":true,"path":"2019-04-25-k8s-api-server-security/","link":"","permalink":"https://study4.fun/2019-04-25-k8s-api-server-security/","excerpt":"","text":"集群安全机制的目标 隔离性，限制容器给集群带来的副作用 最小权限原则 组件的边界划分需要明确 角色划分和权限分配 关于 API ServerAPI Server 作为集群控制请求的实际入口，通常暴露了两个端口——本地端口(Localhost Por)和安全端口(Secure Port)。 本地端口 安全端口 使用场景 测试和启动时使用，Master 节点中不同组件通信 任意场景 安全协议 无 TLS 端口 默认 8080， insecure-port修改 | 默认 6443，secure-port修改 || IP | 默认 localhost,insecure-bind-address修改 | 默认为第一个非 localhost 网卡地址，bind-address修改 || 处理流程 | 无需认证和授权 | 需要认证和授权 || 准入控制 | 是 | 是 || 访问控制 | 需要拥有主机访问权限 | 需要认证和授权模块正常运行 | 注：TLS 中，证书和私钥相关参数为tls-cert-file 和 tls-private-key-file。访问 API Server 的方式有 kubectl，客户端的库和 Rest 请求。通常来说，想在外部访问 API Server 需要通过安全端口访问。通过安全端口访问需要经过三重校验，即 Authentication（身份认证），Authorization（授权）和 Admission Control（准入控制）。 Authentication（身份认证） Authentication is the act of confirming the truth of an attribute of a single piece of data claimed true by an entity.From: https://en.wikipedia.org/wiki/Authentication通俗的来说，身份认证解决的是“让系统/服务端知道你是谁”的问题，即对用户身份的确认。值得注意的是，Wikipedia 中还特别标注“ Not to be confused with authorization. ”对于身份的认证，现实生活中可能是查验证件，也可能是对暗号。 对应的，在 Kubernetes 中，也有对应的几种客户端身份认证方式： 证书认证：基于 CA 根证书签名的双向数字证书认证方式； Token 认证：通过 Token 来认证； 用户密码：通过用户密码的方式认证； 可以同时指定多个身份认证模块，在流程中将会以顺序执行的方式进行认证过程，直到其中一个认证模块认证成功。如果请求认证失败，则会返回 401 状态码。（PS：这里引入了一个 401 状态码的历史遗留问题——401 的语义其实应该是 Unauthenticated） 一旦认证成功，用户就会被分配一个特定的 username。在随后的访问控制流程中，这个 username 将会一直使用。尽管如此，这个 username 却也不会对应存在一个真实的用户对象，该信息也不会被存储。 （我的理解：这个所谓的 username 的存在仅仅是为了在整个访问控制流程中能够进行上下文信息的传递，完成一个链式的验证。和传统意义上的用户相比，K8S 的访问控制基于单次的请求，在请求的过程中抽象来决定行为的合法性和有效性。） Authorization（授权） Authorization is the function of specifying access rights/privileges to resources, which is related to information security and computer security in general and to access control in particular.From: https://en.wikipedia.org/wiki/Authorization与身份认证不同的是，授权关注的是对资源的访问控制，通俗的说就是“系统要知道你这个身份能够做什么”。 当请求通过了身份认证之后，请求才会进入授权流程。请求的内容包含三个部分：用户名（username of the requester），动作（ requested action），动作影响的对象（the object affected by the action）。在已有的多种授权策略中，只要有一个能够声明此用户有执行对应动作的权限，则请求就被授权成功。若所有授权策略全部失败，则返回 403 状态码。授权模块的种类： ABAC RBAC Node Webhook Admission Control（准入控制）通过认证和授权流程之后，请求的调用还需要通过准入控制链的检查。与上述模块不同，准入控制能够修改请求参数完成一些任务。当多个准入控制模块配置完毕后，请求的调用会依次按顺序进行检查。一旦任意一个准入控制模块检查不通过，则请求立即被拒绝。而请求完成了所有检查后，会采用相应 API 对象的验证流程对请求进行验证，然后写入对象库。 Once a request passes all admission controllers, it is validated using the validation routines for the corresponding API object, and then written to the object store (shown as step 4). 总结从整体来看，Kubernetes 对于 API Server 的请求主要分为两大部分：内部和外部。内部是指在 master 节点使用 kubectl 命令进行操作，这时，因为是在节点内部操作，因此并不会使用安全端口，直接采用 localhost 这个 ip 上的非安全端口进行访问。而对于 API Server 的外部请求调用（包括 Pod 和 Rest 请求），则需要使用安全端口进行访问。通过安全端口的请求，需要进行严格的三层校验才能调用成功。这就通过确保只执行权限内允许的操作保证了集群操作的安全性。关于每个部分的详细介绍，会单独抽成三部分继续分解。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"},{"name":"API Server","slug":"API-Server","permalink":"https://study4.fun/tags/API-Server/"}]},{"title":"校园网下OpenWrt配置DNS","slug":"校园网下OpenWrt配置DNS","date":"2018-09-09T05:55:00.000Z","updated":"2022-05-21T07:33:17.268Z","comments":true,"path":"2018-09-09-config-dns/","link":"","permalink":"https://study4.fun/2018-09-09-config-dns/","excerpt":"","text":"在上一篇文章的末尾写到，初次配置 OP 系统时候在校内网实现了 IPV6 穿透，但是 DNS 出现了问题。查阅资料后发现与 OP 的反域名劫持保护机制有关系，下面详述一下。 0x02 OP 的反域名劫持保护OP 的反域名劫持保护在默认情况是开开启的，具体设置在/etc/config/dhcp下。 12config dnsmasq option rebind_protection &#39;1&#39; 在反域名劫持保护未关闭的情况下，由于上级 dns 返回的地址是个私有局域网地址，所以被看作是一次域名劫持，从而丢弃了解析的结果。 直接的方法就是将上面的字段值改为0，关闭即可。在 GUI 配置界面等同于将Network-&gt;DHCP DNS-Server Settings-&gt;General Settings-&gt;Rebind protection的勾取消掉。 再仔细查看文档发现也可以通过白名单的方式放行想要解析的内网域名，更为安全,此时 Rebind protection 也是处于开启状态，上面的关闭操作不需要进行。而具体修改的操作示例如下所示： 123config dnsmasq list rebind_domain &#39;bupt.edu.cn&#39; list rebind_domain &#39;byr.cn&#39; 表示在反域名劫持保护情况下，将bupt.edu.cn,byr.cn域名加入白名单，允许返回内网地址。在 GUI 配置界面等同于在Network-DHCP DNS-Server Settings-General Settings-Domain whitelist添加想要解析的内网域名。 0x03 自定义 DNS 规则在学校 DNS 偶尔抽风或者速度慢的情况下，产生了自定义 DNS 的想法。由于教务系统等系统的访问需要，在各个客户端修改 hosts 略显麻烦，并且 DNSmasq 亦可以实现广告屏蔽，因此采用 DNSmasq 来实现不同的 DNS 解析。预期需求为bupt.edu.cn, byr.cn域名使用校内 DNS 解析，其他地址使用公共 DNS 解析（以 114 为例）。 修改 Wan 口 DNSWan 口 DNS 主要控制路由器访问网络使用的 DNS 服务器。例如，路由器安装软件需要访问网络，那么所使用的 DNS 服务器就是这个。 在/etc/config/network文件中的 wan 接口添加两行 peerdns 以及 dns 字段： 12345config interface &#39;wan&#39; ...... option peerdns &#39;0&#39; option dns &#39;114.114.114.114&#39; ...... 重启 network 服务后生效。在 GUI 配置界面等同于在Network-Interface-Wan-Edit-Common Configuration-Advanced Settings中取消Use DNS servers advertised by peer的勾选，并在Use custom DNS servers添加默认的 DNS 服务器。 修改 Lan 口 DNS（可跳过）LAN 口 DNS 主要控制连接到路由器的设备使用的 DNS。例如，连到路由的电脑上网时使用的 DNS 服务器就在这里设置。 一般情况下，Lan 和 Wan 口 DNS 保持一致即可。如若有需要，修改/etc/config/dhcp文件中 dnsmasq 的 resolvfile 指向即可： 1234config dnsmasq .... option resolvfile &#39;&#x2F;etc&#x2F;resolv.dnsmasq.conf&#39; .... 同时需要在/etc/resolv.dnsmasq.conf下新建对应的配置文件。示例如下： 12nameserver 114.114.114.114nameserver 2001:4860:4860::8888 修改 Dns 自定义的内网解析规则接下来就是配置校内域名使用的 DNS 解析地址。修改/etc/config/dhcp文件中 dnsmasq。首先是删除下面两行配置： 12345config dnsmasq .... option filterwin2k &#39;0&#39; option nonegcache &#39;0&#39; .... 其次添加 list server 字段，对bupt.edu.cn, byr.cn相关域名使用校内 DNS 解析： 123config dnsmasq list server &#39;&#x2F;bupt.edu.cn&#x2F;10.3.9.5&#39; list server &#39;&#x2F;byr.cn&#x2F;10.3.9.5&#39; 在 GUI 配置界面等同于在Network-DHCP and DNS-Server Settings-General Settings-DNS forwardings添加对应域名的 DNS 解析服务器地址。 0x04 后话DNSmasq 的用法远不止于此，可用来内网域名 IP 映射代替 hosts，自定义域名解析规则屏蔽广告等，有时间会再研究。","categories":[{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/categories/%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"网络配置","slug":"网络配置","permalink":"https://study4.fun/tags/%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"},{"name":"DNS","slug":"DNS","permalink":"https://study4.fun/tags/DNS/"}]},{"title":"Kubernetes学习笔记——Pod调度","slug":"Kubernetes学习笔记——Pod调度","date":"2018-09-09T05:55:00.000Z","updated":"2022-05-21T07:33:17.228Z","comments":true,"path":"2018-09-09-k8s-pod-scheduling/","link":"","permalink":"https://study4.fun/2018-09-09-k8s-pod-scheduling/","excerpt":"","text":"0x01. Deployment/RC 全自动调度效果：在集群内始终维持用户指定的副本数量 使用：spec.replicas 原理：系统自动调度算法。由 Master 的 Scheduler 经过一系列算法计算得出，用户无法干预调度过程和结果。 0x02. NodeSelector效果：通过 Node 的标签和 Pod 的 nodeSelector 属性进行匹配，将 Pod 调度到指定的 Node 上 使用： 为目标 Node 打标签 1kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt; 在 Pod 定义中加上 nodeSelector 的设置 1234#pod.yaml---nodeSelector: &lt;label-key&gt;: &lt;your-selected-label-name&gt; 补充： 如果多个 Node 定义了相同的标签，则会根据调度算法从这组 Node 中挑选一个可用的 Node 进行调度 若 Pod 指定的 nodeSelector 条件集群中不存在符合的节点，则该 Pod 无法被成功调度，即使集群中还有可用的 Node 0x03. 亲和性调度篇幅原因，另外一篇单独记录 0x04. 污点(Taints)和容忍(Tolerations)效果：Pod 无法在标记了 Taint 属性的节点上运行, 同时，设置了 Tolerations 的 Pod 可以运行在标注了 Taint 的 Node 上 使用： 为 Node 设置 Taint 信息 1kubectl taint nodes node1 key1=value1:NoSchedule 在 Pod 的配置文件中配置 tolerations 属性 12345tolerations: - key: \"key\" operator: \"Equal\" value: \"value\" effect: \"NoSchdule\" 补充： Taint 和 Toleration 声明需要保持对应一致，且 operator 需要为 Exists 或者 Equal（Equal 需要指定相等 value）； 空 key 配合 Exists 能够匹配所有键值，空 effect 匹配所有 effect； effect 取值也可以设置为 Prefer，例如 PreferNoSchedule，视为软限制； 同个 Node 可以设置多个 Taint，对应的，Pod 也可以设置多个 Toleration。 0x05. DaemonSet效果：在每个 Node 上调度运行同一个（种）Pod，例如日志采集、性能监控、存储的 Daemon 进程 使用： 12apiVersion: extensions/v1beta1kind: DaemonSet 补充： 除了使用系统内置算法在每台 Node 上调度外，也可以在 Pod 定义中使用 NodeSelector 或者 NodeAffinity 来指定满足料件的 Node 范围进行调度 0x06. 批处理调度效果：并行/串行启动多个计算进程去处理一批工作项（Work Item，下称 WI），处理完成后，批处理任务结束 任务模式分类： Job Template Expansion 模式：Job 和 WI 一对一对应，适用于 WI 数量少，但是单 WI 处理数据量大的场景； Queue with Pod Per Work Item 模式：使用一个任务队列存放 WI，Job 作为 Consumer 去完成 WI（对应的，Job 会启动多个 Pod，每个 Pod 对应一个 WI），可用 MQ 实现； Queue with Variable Pod Count 模式：与 2 模式类似，但是 Job 启动的 Pod 数量是可变的，可用 Redis 或数据库实现； Single Job with Static Work Assignment 模式：一个 Job 产生多个 Pod，但是采用程序静态方式分配任务（Kubernetes 不支持，书中所写）。 Job 分类： Non-parallel Jobs：一个 Job 启动一个 Pod，Pod 正常结束则 Job 结束。 Parallel Jobs with a fixed completion count：Job 会启动多个 Pod（数目为 spec.completions），正常结束的 Pod 达到这个数目后 Job 结束。spec.parallelism 可以用来控制并行度。 Parallel Jobs with a work queue：WI 在 Queue 中存放，无法设置并行度参数。每个 Pod 都能够独立判断是否还有任务需要处理，同时，一个 Pod 成功结束则其他 Pod 必定处于即将结束、退出的状态，且 Job 不会再启动新的 Po）。所有 Pod 结束，且至少一个 Pod 成功结束则 Job 算成功结束。 （个人理解：上述的规则说明其实是在说所有 Pod 表现为同一整体，Pod 启动失败会重启是一种容错机制。然而从整个过程的跨度来看，无需关心失败启动的数目，只要不是所有 Pod 全部失败结束，只需存在一个成功结束的 Pod 即表明 Job 流程内的其他划分任务都正常完成，整体任务也已成功完成。） 0x07. 定时任务效果：定期触发任务执行 使用： 在 API Server 启动进程上添加配置参数 1--runtime-config=batch/v2alpha1=true 编写 Cron Job 配置文件 123456#cron.yamlapiVersion: batch/v2alpha1kind: CronJob---spec: schedule: \"*/1 * * * *\" schedule 格式如下 1Min Hour DayOfMonth Month DayOfWeek *表示任意值，即每个时间单元节点都会触发 /表示开始触发的时间，例如 5/20，表明第一次触发在第 5 个时间单位，此后每隔 20 个时间单位触发 0x08. 自定义调度器在 Pod 中提供自定义的调度器名称，则默认调度器就会失效，转而使用指定的调度器完成对应 Pod 的调度，自定义的调度器需要通过 kube-proxy 来运行，如果自定义调度器始终未启动，则 Pod 将会卡 Pending 状态。 12345apiVersion: v1kind: Pod---spec: schedulerName: my-scheduler 0x09. 补充 Admission controller 需要仔细研究 TaintBasedEviction 和 Eureka 中的驱逐机制（包括 SELF PRESERVATION)是否在设计层面上有一定的共通点 自定义调度器实现有时间需要手动验证一次","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"},{"name":"Pod","slug":"Pod","permalink":"https://study4.fun/tags/Pod/"}]},{"title":"双栈网络配置路由器的ipv6穿透","slug":"双栈网络配置路由器的ipv6穿透","date":"2016-03-20T14:47:00.000Z","updated":"2022-05-21T07:33:17.304Z","comments":true,"path":"2016-03-20-config-ipv6/","link":"","permalink":"https://study4.fun/2016-03-20-config-ipv6/","excerpt":"","text":"本教程测试过程基于 Newifi Y2 路由器，系统为 PandoraBox，理论上 OpenWrt 原生同样适用。 1.修改路由器的软件源123456789101112dest root &#x2F;dest ram &#x2F;tmplists_dir ext &#x2F;var&#x2F;opkg-listsoption overlay_root &#x2F;overlaysrc&#x2F;gz 14.09_base http:&#x2F;&#x2F;downloads.openwrt.org.cn&#x2F;PandoraBox&#x2F;ralink&#x2F;packages&#x2F;basesrc&#x2F;gz 14.09_telephony http:&#x2F;&#x2F;downloads.openwrt.org&#x2F;chaos_calmer&#x2F;15.05&#x2F;ramips&#x2F;mt7620&#x2F;packages&#x2F;telephonysrc&#x2F;gz 14.09_packages http:&#x2F;&#x2F;downloads.openwrt.org&#x2F;chaos_calmer&#x2F;15.05&#x2F;ramips&#x2F;mt7620&#x2F;packages&#x2F;packagessrc&#x2F;gz 14.09_routing http:&#x2F;&#x2F;downloads.openwrt.org&#x2F;chaos_calmer&#x2F;15.05&#x2F;ramips&#x2F;mt7620&#x2F;packages&#x2F;routingsrc&#x2F;gz 14.09_management http:&#x2F;&#x2F;downloads.openwrt.org&#x2F;chaos_calmer&#x2F;15.05&#x2F;ramips&#x2F;mt7620&#x2F;packages&#x2F;managementarch ralink 1arch all 2arch ramips_24kec 3 注:软件源由于硬件配置不同的会有所区别，Newifi 是 MT7620 方案，其他芯片方案的请移步以下两个网址自行匹配： OpenWrt 中文网址 http://downloads.openwrt.org.cn/OpenWrt download area https://downloads.openwrt.org/ 2.配置 Odhcpd 软件包中更新并安装最新版本的 odhcpd 修改 dhcp 文件（文件路径：/etc/config/dhcp，不熟悉 ssh 的可以使用 winscp 修改) 123456789101112131415161718192021222324252627282930313233343536373839404142config dnsmasq option domainneeded &#39;1&#39; option boguspriv &#39;1&#39; option filterwin2k &#39;0&#39; option localise_queries &#39;1&#39; option rebind_protection &#39;1&#39; option rebind_localhost &#39;1&#39; option local &#39;&#x2F;lan&#x2F;&#39; option domain &#39;lan&#39; option expandhosts &#39;1&#39; option nonegcache &#39;0&#39; option authoritative &#39;1&#39; option readethers &#39;1&#39; option leasefile &#39;&#x2F;tmp&#x2F;dhcp.leases&#39; option resolvfile &#39;&#x2F;tmp&#x2F;resolv.conf.auto&#39; option localservice &#39;1&#39;config dhcp &#39;lan&#39; option interface &#39;lan&#39; option start &#39;100&#39; option limit &#39;150&#39; option leasetime &#39;12h&#39; option dhcpv6 &#39;hybrid&#39; option ra &#39;hybrid&#39; option ndp &#39;hybrid&#39; option ra_management &#39;1&#39;config dhcp &#39;wan&#39; option interface &#39;wan&#39; option ignore &#39;1&#39;config odhcpd &#39;odhcpd&#39; option maindhcp &#39;0&#39; option leasefile &#39;&#x2F;tmp&#x2F;hosts&#x2F;odhcpd&#39; option leasetrigger &#39;&#x2F;usr&#x2F;sbin&#x2F;odhcpd-update&#39;config dhcp &#39;wan6&#39; option interface &#39;wan&#39; option dhcpv6 &#39;hybrid&#39; option ra &#39;hybrid&#39; option ndp &#39;hybrid&#39; option master &#39;1&#39; 修改后保存并重启路由器即可。 3.后话配置后好像 dns 出了一些问题，在访问其他校内以.byr.cn 或.bupt.edu.cn 为后缀的网址显示 dns 错误，如果有大牛解决了这个 DNS 问题，可以分享一下思路。 2018.9.9 更新：后话所述问题已经解决","categories":[{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/categories/%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"网络配置","slug":"网络配置","permalink":"https://study4.fun/tags/%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"},{"name":"IPV6","slug":"IPV6","permalink":"https://study4.fun/tags/IPV6/"}]}],"categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/categories/%E6%9D%82%E9%A1%B9/"},{"name":"备忘录","slug":"备忘录","permalink":"https://study4.fun/categories/%E5%A4%87%E5%BF%98%E5%BD%95/"},{"name":"日记","slug":"日记","permalink":"https://study4.fun/categories/%E6%97%A5%E8%AE%B0/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"},{"name":"Pod","slug":"Pod","permalink":"https://study4.fun/tags/Pod/"},{"name":"容器","slug":"容器","permalink":"https://study4.fun/tags/%E5%AE%B9%E5%99%A8/"},{"name":"监控","slug":"监控","permalink":"https://study4.fun/tags/%E7%9B%91%E6%8E%A7/"},{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"KVM","slug":"KVM","permalink":"https://study4.fun/tags/KVM/"},{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/tags/%E6%9D%82%E9%A1%B9/"},{"name":"cgroup","slug":"cgroup","permalink":"https://study4.fun/tags/cgroup/"},{"name":"Unix","slug":"Unix","permalink":"https://study4.fun/tags/Unix/"},{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"设计模式","slug":"设计模式","permalink":"https://study4.fun/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Golang","slug":"Golang","permalink":"https://study4.fun/tags/Golang/"},{"name":"API Server","slug":"API-Server","permalink":"https://study4.fun/tags/API-Server/"},{"name":"网络配置","slug":"网络配置","permalink":"https://study4.fun/tags/%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"},{"name":"DNS","slug":"DNS","permalink":"https://study4.fun/tags/DNS/"},{"name":"IPV6","slug":"IPV6","permalink":"https://study4.fun/tags/IPV6/"}]}